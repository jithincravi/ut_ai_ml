{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The healthcare industry is rapidly evolving, with professionals facing increasing challenges in managing vast volumes of medical data while delivering accurate and timely diagnoses. The need for quick access to comprehensive, reliable, and up-to-date medical knowledge is critical for improving patient outcomes and ensuring informed decision-making in a fast-paced environment.\n",
    "\n",
    "Healthcare professionals often encounter information overload, struggling to sift through extensive research and data to create accurate diagnoses and treatment plans. This challenge is amplified by the need for efficiency, particularly in emergencies, where time-sensitive decisions are vital. Furthermore, access to trusted, current medical information from renowned manuals and research papers is essential for maintaining high standards of care.\n",
    "\n",
    "To address these challenges, healthcare centers can focus on integrating systems that streamline access to medical knowledge, provide tools to support quick decision-making, and enhance efficiency. Leveraging centralized knowledge platforms and ensuring healthcare providers have continuous access to reliable resources can significantly improve patient care and operational effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Questions to Answer**\n",
    "\n",
    "**1. Diagnostic Assistance**: \"What are the common symptoms and treatments for pulmonary embolism?\"\n",
    "\n",
    "**2. Drug Information**: \"Can you provide the trade names of medications used for treating hypertension?\"\n",
    "\n",
    "**3. Treatment Plans**: \"What are the first-line options and alternatives for managing rheumatoid arthritis?\"\n",
    "\n",
    "**4. Specialty Knowledge**: \"What are the diagnostic steps for suspected endocrine disorders?\"\n",
    "\n",
    "**5. Critical Care Protocols**: \"What is the protocol for managing sepsis in a critical care unit?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an AI specialist, your task is to develop a RAG-based AI solution using renowned medical manuals to address healthcare challenges. The objective is to **understand** issues like information overload, **apply** AI techniques to streamline decision-making, **analyze** its impact on diagnostics and patient outcomes, **evaluate** its potential to standardize care practices, and **create** a functional prototype demonstrating its feasibility and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Merck Manuals** are medical references published by the American pharmaceutical company Merck & Co., that cover a wide range of medical topics, including disorders, tests, diagnoses, and drugs. The manuals have been published since 1899, when Merck & Co. was still a subsidiary of the German company Merck.\n",
    "\n",
    "The manual is provided as a PDF with over 4,000 pages divided into 23 sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnwETBOE6Bz5"
   },
   "source": [
    "## Installing and Importing Necessary Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4GgLhZhUM4V",
    "outputId": "5199a58a-d12e-4f69-dd31-4de62a8c1bfa"
   },
   "outputs": [],
   "source": [
    "# Installation for GPU llama-cpp-python\n",
    "# uncomment and run the following code in case GPU is being used\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=ON\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Run the above installation code only once.**\n",
    "- After installation, check if the following file exists:\n",
    "  \n",
    "  ```\n",
    "  .venv/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal\n",
    "  ```\n",
    "\n",
    "- **Note:** Regular `pip install llama-cpp-python` will **not** enable GPU support. You must install it as shown above for GPU acceleration.\n",
    "\n",
    "- You may see some errors related to `numpy`. These can be ignored for now; we will address them in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To install all required dependencies for this notebook, run the following command in your terminal:**\n",
    "\n",
    "`pip install -r requirements_rag.txt`\n",
    "\n",
    "or install your versions of libraries using !pip install\n",
    "\n",
    "This will fix the numpy errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Restart Kernel after running the above code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RTY9GN4oWK3g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jithinravi/work/github/ut_ai_ml/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Libraries for processing dataframes,text\n",
    "import json,os\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "\n",
    "#Libraries for Loading Data, Chunking, Embedding, and Vector Databases\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#Libraries for downloading and loading the llm\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtZWqj0wFTS1"
   },
   "source": [
    "## Question Answering using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq1lhM4WFTS2"
   },
   "source": [
    "#### Downloading and Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.8 (v3.12.8:2dc476bcb91, Dec  3 2024, 14:43:19) [Clang 13.0.0 (clang-1300.0.29.30)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dA3XQMWmQLJp"
   },
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "    filename=\"mistral-7b-instruct-v0.2.Q6_K.gguf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jithinravi/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q6_K.gguf\n"
     ]
    }
   ],
   "source": [
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation for GPU llama-cpp-python\n",
    "# uncomment and run the following code in case GPU is being used\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=OFF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/jithinravi/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q6_K.gguf (version unknown)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:              blk.8.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:              blk.9.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.10.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.11.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:             blk.12.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.17.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:             blk.26.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:             blk.31.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32     \n",
      "llama_model_loader: - kv  11:                          general.file_type u32     \n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32     \n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool    \n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool    \n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str     \n",
      "llama_model_loader: - kv  23:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_print_meta: format         = unknown\n",
      "llm_load_print_meta: arch           = llama\n",
      "llm_load_print_meta: vocab type     = SPM\n",
      "llm_load_print_meta: n_vocab        = 32000\n",
      "llm_load_print_meta: n_merges       = 0\n",
      "llm_load_print_meta: n_ctx_train    = 32768\n",
      "llm_load_print_meta: n_ctx          = 4096\n",
      "llm_load_print_meta: n_embd         = 4096\n",
      "llm_load_print_meta: n_head         = 32\n",
      "llm_load_print_meta: n_head_kv      = 8\n",
      "llm_load_print_meta: n_layer        = 32\n",
      "llm_load_print_meta: n_rot          = 128\n",
      "llm_load_print_meta: n_gqa          = 4\n",
      "llm_load_print_meta: f_norm_eps     = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps = 1.0e-05\n",
      "llm_load_print_meta: n_ff           = 14336\n",
      "llm_load_print_meta: freq_base      = 1000000.0\n",
      "llm_load_print_meta: freq_scale     = 1\n",
      "llm_load_print_meta: model type     = 7B\n",
      "llm_load_print_meta: model ftype    = mostly Q6_K\n",
      "llm_load_print_meta: model size     = 7.24 B\n",
      "llm_load_print_meta: general.name   = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: mem required  = 5666.19 MB (+  512.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: kv self size  =  512.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_init: loading '/Users/jithinravi/work/github/ut_ai_ml/.venv/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x108ee2e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                        0x108ee31f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                            0x108ee3590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x108ee3960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                          0x108ee3d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                           0x108ee4100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                           0x108ee44d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                           0x108ee48a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x108ee4c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_4                     0x108ee5040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x108ee5410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                0x108ee57e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x108ee5bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x108ee5f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x108ee6350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                  0x108ee6720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x108ee6af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x108ee6ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x108ee7290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x108ee7660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x108ee7a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x108ee7e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                           0x108ee81d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x108ee85a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32_1row           0x108ee8970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32_l4             0x108ee8d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x108ee9110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x108ee94e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q8_0_f32               0x108ee98b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x108ee9c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x108eea050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x108eea420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x108eea7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x108eeabc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x108eeaf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x108eeb360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                0x108eeb730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x108eebb00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x108eebed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x108eec2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x108eec670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x108eeca40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x108eece10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope                           0x108eed1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x108eed5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x108eed980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x108eedd50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x108eee120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 16384.02 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =  285.47 MB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  5666.80 MB, (12133.80 / 16384.02)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.48 MB, (12135.28 / 16384.02)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   514.00 MB, (12649.28 / 16384.02)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   284.02 MB, (12933.30 / 16384.02)\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=10,  # CPU cores\n",
    "    n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=43,  # uncomment and change this value based on GPU VRAM pool.\n",
    "    n_ctx=4096,  # Context window\n",
    "verbose=False,  # Reduce verbose output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzzkvIXvFTS4"
   },
   "source": [
    "#### Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hG_IaZj0QLw4"
   },
   "outputs": [],
   "source": [
    "def response(query,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
    "    model_output = lcpp_llm (\n",
    "      prompt=query,\n",
    "      max_tokens=max_tokens,\n",
    "      temperature=temperature,\n",
    "      top_p=top_p,\n",
    "      top_k=top_k\n",
    "    )\n",
    "    #print(\"Model Output:\", model_output)\n",
    "    return model_output['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8YgK91SFjVY"
   },
   "source": [
    "### Query 1: What is the protocol for managing sepsis in a critical care unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-JLIVmpPQH0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "\n",
      "Sepsis is a life-threatening condition that can arise from an infection, and it requires prompt recognition and aggressive management in a critical care unit. The following are general steps for managing sepsis in a critical care unit:\n",
      "\n",
      "1. Early recognition: Recognize the signs and symptoms of sepsis early and initiate treatment as soon as possible. Sepsis can present with various clinical features, including fever or hypothermia, tachycardia or bradycardia, altered mental status, respiratory distress, and lactic acidosis.\n",
      "2. Source control\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is the protocol for managing sepsis in a critical care unit?\"\n",
    "response_text1 = response(query1)\n",
    "print(\"Response:\", response_text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6yxICeVFjVc"
   },
   "source": [
    "### Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BdiHRgEqQIP9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "\n",
      "Appendicitis is a medical condition characterized by inflammation of the appendix, a small tube-shaped organ located in the lower right side of the abdomen. The symptoms of appendicitis can vary from person to person, but some common signs include:\n",
      "\n",
      "1. Abdominal pain: The pain is typically located in the lower right quadrant of the abdomen and may start as a mild discomfort that gradually worsens over time. The pain may be constant or intermittent and may be aggravated by movement, deep breathing, or coughing.\n",
      "2. Loss of appetite\n"
     ]
    }
   ],
   "source": [
    "query2 = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\n",
    "response2_text = response(query2)\n",
    "print(\"Response:\", response2_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oflaoOGiFjVd"
   },
   "source": [
    "### Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "N-mx9yboQIt-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "\n",
      "Sudden patchy hair loss, also known as alopecia areata, is a common autoimmune disorder that affects the hair follicles. It can result in round or oval bald patches on the scalp, but it can also occur on other parts of the body such as the beard area, eyebrows, or eyelashes.\n",
      "\n",
      "The exact cause of alopecia areata is not known, but it's believed to be related to a problem with the immune system. Some possible triggers for this condition include stress, genetics, viral infections, and certain medications.\n"
     ]
    }
   ],
   "source": [
    "query3 = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\n",
    "response3_text = response(query3)\n",
    "print(\"Response:\", response3_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUUqY4FbFjVe"
   },
   "source": [
    "### Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TEsVMaKaQJzh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "\n",
      "A person who has sustained a physical injury to the brain tissue may require various treatments depending on the severity and location of the injury. Here are some common treatments that may be recommended:\n",
      "\n",
      "1. Emergency care: In case of a traumatic brain injury (TBI), it is essential to seek emergency medical attention as soon as possible. The primary goal of emergency care is to prevent further damage to the brain, stabilize vital signs, and manage any life-threatening conditions.\n",
      "2. Medications: Depending on the symptoms, healthcare professionals may prescribe medications to manage various conditions associated with a\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\n",
    "response4_text = response(query4)\n",
    "print(\"Response:\", response4_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5laPFTHrFjVf"
   },
   "source": [
    "### Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VfrlmrP5QKJz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "\n",
      "First and foremost, if you suspect that someone has fractured their leg while hiking, it's essential to ensure their safety and prevent further injury. Here are some necessary precautions:\n",
      "\n",
      "1. Keep the person calm and still: Encourage them to remain as still as possible to minimize pain and prevent worsening the injury.\n",
      "2. Assess the situation: Check for any signs of shock, such as pale skin, rapid heartbeat, or shallow breathing. If you notice these symptoms, seek medical help immediately.\n",
      "3. Immobilize the leg: Use a splint, sl\n"
     ]
    }
   ],
   "source": [
    "query5 = \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\n",
    "response5_text = response(query5)\n",
    "print(\"Response:\", response5_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - First Approach - Plain LLM response\n",
    "\n",
    "- **Initial Approach:**  \n",
    "    The plain LLM model is used for question answering without any prompt engineering or parameter tuning.\n",
    "\n",
    "- **Default Model Behavior:**  \n",
    "    The model responds to queries using its pre-trained knowledge, without any additional guidance or customization.\n",
    "\n",
    "- **Limitations:**  \n",
    "    - While this method is straightforward and quick to implement, it may not yield the most accurate or contextually relevant answers—especially for specialized domains like medicine. Without tuning parameters (e.g., `temperature`, `max_tokens`) or providing system/user prompts, the model's outputs can be generic and may lack the specificity or structure required for clinical use. \n",
    "    - We also see the answers are largely `truncated` which is not ideal.\n",
    "\n",
    "- **Advanced Techniques:**  \n",
    "    More advanced methods, such as prompt engineering or Retrieval-Augmented Generation (RAG), can significantly improve the quality and reliability of responses in such applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5myZ5dOOefc"
   },
   "source": [
    "## Question Answering using LLM with Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of prompt engineering for better LLM responses\n",
    "\n",
    "# Define a system prompt to instruct the LLM to answer as a medical expert using evidence from the Merck Manual\n",
    "qna_system_message = (\n",
    "    \"You are a highly knowledgeable medical assistant. \"\n",
    "    \"Be concise, accurate. If you don't know the answer, say 'I don't know'. \"\n",
    "    \"Not more than 4 points in your response.No more than approximately 30 words in each point. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Jg3r_LWOeff"
   },
   "source": [
    "### Query 1: What is the protocol for managing sepsis in a critical care unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Immediate antibiotic administration based on suspected infection source and sensitivity results.\n",
      "2. Aggressive fluid resuscitation to maintain adequate tissue perfusion.\n",
      "3. Close monitoring of vital signs, lactate levels, and organ function.\n",
      "4. Consideration of vasopressors or mechanical ventilation as needed for hemodynamic instability.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe1 = response(qna_system_message + \" \" + query1, max_tokens=256).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "We added a system prompt to guide the model's responses and used the `max_tokens` parameter to ensure the responses are not truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYpyw4HjOeff"
   },
   "source": [
    "### Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Appendicitis: Sudden pain around navel, loss of appetite, fever, abdominal swelling, and vomiting.\n",
      "2. No cure with medicine: Appendicitis requires surgery to remove the inflamed appendix before it bursts and spreads infection.\n",
      "3. Surgical procedure: Laparoscopic appendectomy is a common surgical procedure used to remove the appendix through small incisions, minimizing recovery time.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe2 = response(qna_system_message + \" \" + query2, max_tokens=256).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GXl09pFfRPBr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Abdominal pain, especially around the navel that migrates to the lower right side\n",
      "\n",
      "2. Loss of appetite\n",
      "3. Fever\n",
      "4. Vomiting or feeling sick (nausea)\n",
      "\n",
      "Applying heat or antibiotics may relieve mild symptoms but won't cure appendicitis. The only treatment is appendectomy - surgery to remove the infected appendix.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe2 = response(qna_system_message + \" \" + query2, max_tokens=256, temperature=5).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Increasing the temperature parameter has made the model's responses more non-deterministic. This is evident in the following:  \n",
    "\n",
    "- Some points in the response are overly brief, sometimes consisting of just a single word. Sometimes not answering the question at all \n",
    "\n",
    "This behavior highlights the trade-off between creativity and consistency when adjusting the temperature setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRp92JQZOeff"
   },
   "source": [
    "### Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Minoxidil: Over-the-counter topical treatment that stimulates hair growth and slows down hair loss in affected areas.\n",
      "2. Corticosteroids: Prescription medication to reduce inflammation and suppress immune system response causing alopecia areata.\n",
      "3. DHT blockers: Finasteride or dutasteride can inhibit the production of dihydrotestosterone, a hormone contributing to male pattern baldness.\n",
      "4. Hair transplant: Surgical procedure where healthy hair follicles are moved from donor areas to bald spots for permanent regrowth.\n",
      "\n",
      "Possible causes: autoimmune disorders, stress, genetics, nutritional deficiencies, certain medications, or infections. Consult a healthcare professional for proper diagnosis and treatment plan.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe3 = response(qna_system_message + \" \" + query3, max_tokens=256).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JOgATEpMRPve"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Minoxidil: Over-the-counter topical treatment that stimulates hair growth, effective for androgenetic alopecia and some types of patchy hair loss.\n",
      "2. Corticosteroids: Prescription medication to reduce inflammation and suppress the immune system, useful for treating alopecia areata, an autoimmune cause of sudden hair loss.\n",
      "3. Hair transplant surgery: Permanent solution for bald spots caused by androgenetic alopecia or other forms of permanent hair loss.\n",
      "4. Nutritional deficiencies: Correcting vitamin or mineral deficiencies, such as iron or biotin, can help improve hair growth and potentially reverse patchy hair loss.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe3 = response(qna_system_message + \" \" + query3, max_tokens=256, temperature= 5, top_p=0.4).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- When `top_p` is set low (e.g., 0.4), the model only considers the most probable tokens, often resulting in shorter, less detailed, or less varied answers. This can cause the model to omit less common details—like \"possible causes\"—and focus only on the most typical treatments or facts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA45zwyUOefg"
   },
   "source": [
    "### Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Rehabilitation therapy: Occupational, speech, and physical therapy help improve function and compensate for deficits.\n",
      "2. Medications: Anti-seizure drugs, pain relievers, and psychotropic medications may be prescribed to manage symptoms.\n",
      "3. Surgery: Depending on the injury's location and severity, surgery might be necessary to remove hematomas or repair damaged tissue.\n",
      "4. Lifestyle modifications: Rest, proper nutrition, and stress management can aid in recovery and help maintain brain health.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe4 = response(qna_system_message + \" \" + query4, max_tokens=256 ).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VA7G8FOnRQZY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Rehabilitation therapy: Occupational, speech, and physical therapy help improve function and compensate for deficits.\n",
      "2. Medications: Anti-seizure drugs, pain relievers, and psychotropic medications may be prescribed to manage symptoms.\n",
      "3. Surgery: Depending on the injury's location and severity, surgery might be necessary to remove hematomas or repair damaged tissue.\n",
      "4. Lifestyle modifications: Rest, proper nutrition, and stress management can aid in recovery and help maintain brain health.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe4 = response(qna_system_message + \" \" + query4, max_tokens=256, top_k=1).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- In our case the model is generating deterministic responses, changing `top_k` may not have any effect. (mostly because we set temp to zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYXxiSuBOefg"
   },
   "source": [
    "### Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mE2GMQk8RQ_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Immediate first aid: Apply a sterile dressing to the wound, immobilize the leg with a splint or sling, and control bleeding if necessary.\n",
      "2. Seek medical attention: Transport the person to the nearest hospital or urgent care center for proper diagnosis and treatment of the fracture.\n",
      "3. Pain management: Administer over-the-counter pain medication or prescription painkillers as directed by a healthcare professional.\n",
      "4. Follow doctor's instructions: Attend follow-up appointments, undergo rehabilitation therapy, and adhere to any weight-bearing restrictions or other guidelines provided by the healthcare team.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe5 = response(qna_system_message + \" \" + query5, max_tokens=256 ).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - Second Approach - After prompt Engineering and fine tuning\n",
    "The chosen parameters for a deterministic model configuration are:\n",
    "\n",
    "- **max_tokens=256**: Limits the response length to a maximum of 256 tokens, ensuring concise and controlled outputs.\n",
    "- **temperature=0**: Eliminates randomness, making the models responses deterministic and consistent.\n",
    "- **top_p=0.95**: Enables nucleus sampling, considering only the top 95% of the probability mass for token selection, ensuring relevance while maintaining some flexibility.\n",
    "- **top_k=50**: Restricts token selection to the top 50 most probable tokens, further refining the response quality.\n",
    "\n",
    "This configuration is ideal for generating precise, repeatable, and contextually accurate outputs, especially in critical applications like medical question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_O1PGdNO2M9"
   },
   "source": [
    "## Data Preparation for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTpWESc53dL9"
   },
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ybj2cEnzRSXq"
   },
   "outputs": [],
   "source": [
    "#load the pdf file\n",
    "loader = PyMuPDFLoader(\"medical_diagnosis_manual.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffj0ca3eZT4u"
   },
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9weTDzMxRRS"
   },
   "source": [
    "#### Checking the first 2 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MSEiL--bRTZT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Content:\n",
      "hovels_bounces.0v@icloud.com\n",
      "Y74OHC3EQZ\n",
      "for personal use by hovels_bounces.0v@\n",
      "shing the contents in part or full is liable \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Page 2 Content:\n",
      "hovels_bounces.0v@icloud.com\n",
      "Y74OHC3EQZ\n",
      "This file is meant for personal use by hovels_bounces.0v@icloud.com only.\n",
      "Sharing or publishing the contents in part or full is liable for legal action.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display the content of the first 2 pages\n",
    "for i, doc in enumerate(documents[:2]):\n",
    "    print(f\"Page {i + 1} Content:\\n{doc.page_content}\\n{'-' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-wNNalNxPKT"
   },
   "source": [
    "#### Checking the number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-NuC-6SNRT7K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in the PDF: 4114\n"
     ]
    }
   ],
   "source": [
    "# Check the number of pages in the loaded PDF document\n",
    "num_pages = len(documents)\n",
    "print(f\"Number of pages in the PDF: {num_pages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LECMxTH-zB-R"
   },
   "source": [
    "### Data Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ir9Zi8rKRUmG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks created: 18088\n",
      "Sample chunk:\n",
      " hovels_bounces.0v@icloud.com\n",
      "Y74OHC3EQZ\n",
      "for personal use by hovels_bounces.0v@\n",
      "shing the contents in part or full is liable\n"
     ]
    }
   ],
   "source": [
    "# Chunk the documents for embedding using RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # Number of characters per chunk\n",
    "    chunk_overlap=200     # Overlap between chunks to preserve context\n",
    ")\n",
    "\n",
    "# Split all documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks created: {len(chunks)}\")\n",
    "print(\"Sample chunk:\\n\", chunks[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunk:\n",
      " 201, 910\n",
      "mastocytosis vs 1125\n",
      "Menetrier's disease vs 132\n",
      "peptic ulcer disease vs 134\n",
      "Zolmitriptan 1721\n",
      "Zolpidem 1709, 3103\n",
      "Zonisamide 1701\n",
      "Zoonotic diseases, cutaneous 718\n",
      "Zoophobia 1498\n",
      "Zoster (see Herpes zoster virus infection)\n",
      "Zygomycosis 1332\n",
      "The Merck Manual of Diagnosis & Therapy, 19th Edition\n",
      "Z\n",
      "4104\n",
      "hovels_bounces.0v@icloud.com\n",
      "Y74OHC3EQZ\n",
      "This file is meant for personal use by hovels_bounces.0v@icloud.com only.\n",
      "Sharing or publishing the contents in part or full is liable for legal action.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample chunk:\\n\", chunks[-1].page_content[-500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvHVejcWz0Bl"
   },
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "R3CAgoUeRVLa"
   },
   "outputs": [],
   "source": [
    "def embed_and_create_chroma(chunks, persist_directory=\"chroma_db\", embedding_model_name=\"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Embeds the provided chunks and creates a persistent Chroma vector store.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of langchain Document objects to embed.\n",
    "        persist_directory (str): Directory to persist the Chroma database.\n",
    "        embedding_model_name (str): Name of the sentence-transformer model to use for embeddings.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: A persistent Chroma vector store instance.\n",
    "    \"\"\"\n",
    "    # Create embedding function\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "    # Create Chroma vector store and persist it\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding_function,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiKCOv4X0d7B"
   },
   "source": [
    "### Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "Matplotlib is building the font cache; this may take a moment.\n",
      "/Users/jithinravi/work/github/ut_ai_ml/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma vector store created and persisted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create and persist the Chroma vector store\n",
    "persist_directory = \"chroma_db\"  # Directory to store the Chroma database\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"  # Name of the embedding model\n",
    "\n",
    "vectordb = embed_and_create_chroma(chunks, persist_directory=persist_directory, embedding_model_name=embedding_model_name)\n",
    "\n",
    "print(\"Chroma vector store created and persisted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks created: 18088\n"
     ]
    }
   ],
   "source": [
    "# Check the number of documents (chunks) stored in the Chroma vector database  vs Original chunks\n",
    "print(f\"Total number of chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks in the vector database: 18088\n"
     ]
    }
   ],
   "source": [
    "# Check the number of documents (chunks) stored in the Chroma vector database\n",
    "print(\"Number of chunks in the vector database:\", vectordb._collection.count())\n",
    "\n",
    "# Optionally, check a few sample chunks stored in the vector database\n",
    "sample_docs = vectordb.get(include=['documents'], ids=[str(i) for i in range(5)])\n",
    "for idx, doc in enumerate(sample_docs['documents']):\n",
    "    print(f\"Chunk {idx}:\\n{doc[:500]}\\n{'-'*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEa5sKc41T1z"
   },
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "wBlQUGx3RWUD"
   },
   "outputs": [],
   "source": [
    "# Create a retriever from the Chroma vector database\n",
    "retriever = vectordb.as_retriever(  search_type='similarity',search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='16 - Critical Care Medicine\\nChapter 222. Approach to the Critically Ill Patient\\nIntroduction\\nCritical care medicine specializes in caring for the most seriously ill patients. These patients are best\\ntreated in an ICU staffed by experienced personnel. Some hospitals maintain separate units for special\\npopulations (eg, cardiac, surgical, neurologic, pediatric, or neonatal patients). ICUs have a high\\nnurse:patient ratio to provide the necessary high intensity of service, including treatment and monitoring\\nof physiologic parameters.\\nSupportive care for the ICU patient includes provision of adequate nutrition (see p. 21) and prevention of\\ninfection, stress ulcers and gastritis (see p. 131), and pulmonary embolism (see p. 1920). Because 15 to\\n25% of patients admitted to ICUs die there, physicians should know how to minimize suffering and help\\ndying patients maintain dignity (see p. 3480).\\nPatient Monitoring and Testing', metadata={'author': '', 'creationDate': 'D:20120615054440Z', 'creator': 'Atop CHM to PDF Converter', 'file_path': 'medical_diagnosis_manual.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': 'D:20250628125758Z', 'page': 2400, 'producer': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'source': 'medical_diagnosis_manual.pdf', 'subject': '', 'title': 'The Merck Manual of Diagnosis & Therapy, 19th Edition', 'total_pages': 4114, 'trapped': ''}),\n",
       " Document(page_content=\"septic shock is high (60 to 65%). Prognosis depends on the cause, preexisting or complicating illness,\\ntime between onset and diagnosis, and promptness and adequacy of therapy.\\nGeneral management: First aid involves keeping the patient warm. Hemorrhage is controlled, airway\\nand ventilation are checked, and respiratory assistance is given if necessary. Nothing is given by mouth,\\nand the patient's head is turned to one side to avoid aspiration if emesis occurs.\\nTreatment begins simultaneously with evaluation. Supplemental O2 by face mask is provided. If shock is\\nsevere or if ventilation is inadequate, airway intubation with mechanical ventilation is necessary. Two\\nlarge (16- to 18-gauge) IV catheters are inserted into separate peripheral veins. A central venous line or\\nan intraosseous needle, especially in children, provides an alternative when peripheral veins cannot\\npromptly be accessed (see also p. 2250).\", metadata={'author': '', 'creationDate': 'D:20120615054440Z', 'creator': 'Atop CHM to PDF Converter', 'file_path': 'medical_diagnosis_manual.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': 'D:20250628125758Z', 'page': 2448, 'producer': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'source': 'medical_diagnosis_manual.pdf', 'subject': '', 'title': 'The Merck Manual of Diagnosis & Therapy, 19th Edition', 'total_pages': 4114, 'trapped': ''}),\n",
       " Document(page_content=\"may be used. Monotherapy with maximal therapeutic doses of ceftazidime (2 g IV q 8 h) or imipenem (1 g\\nIV q 6 h) may be effective but is not recommended.\\nVancomycin must be added if resistant staphylococci or enterococci are suspected. If there is an\\nabdominal source, a drug effective against anaerobes (eg, metronidazole) should be included. When\\nculture and sensitivity results are available, the antibiotic regimen is changed accordingly. Antibiotics are\\ncontinued for at least 5 days after shock resolves and evidence of infection subsides.\\nAbscesses must be drained, and necrotic tissues (eg, infarcted bowel, gangrenous gall-bladder,\\nabscessed uterus) must be surgically excised. The patient's condition will continue to deteriorate despite\\nantibiotic therapy unless septic foci are eliminated.\\nNormalization of blood glucose improves outcome in critically ill patients, even those not known to be\", metadata={'author': '', 'creationDate': 'D:20120615054440Z', 'creator': 'Atop CHM to PDF Converter', 'file_path': 'medical_diagnosis_manual.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': 'D:20250628125758Z', 'page': 2456, 'producer': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'source': 'medical_diagnosis_manual.pdf', 'subject': '', 'title': 'The Merck Manual of Diagnosis & Therapy, 19th Edition', 'total_pages': 4114, 'trapped': ''})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"How to treat a patient with sepsis in a critical care unit?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vw8qcwq66B0C",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### System and User Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "GF_4399TRW5D"
   },
   "outputs": [],
   "source": [
    "# System and user prompt templates for RAG-based medical assistant\n",
    "\n",
    "qna_system_message = (\n",
    "    \"You are a highly knowledgeable medical assistant. \"\n",
    "    \"Use only the provided context from the Merck Manual to answer the user's question. \"\n",
    "    \"Be concise, accurate, and cite relevant sections if possible. \"\n",
    "    \"If the answer is not present in the context, say 'I don't know'. \"\n",
    "    \"Limit your response to a maximum of 4 bullet points, each not exceeding 30 words.\"\n",
    ")\n",
    "\n",
    "qna_user_message_template = (\n",
    "    \"Context:\\n{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    "    \"Answer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkIteX4m6mny"
   },
   "source": [
    "### Response Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "5jFvGnOJRXZx"
   },
   "outputs": [],
   "source": [
    "def generate_rag_response(user_input,k=3,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
    "    global qna_system_message,qna_user_message_template\n",
    "    # Retrieve relevant document chunks\n",
    "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=k)\n",
    "    context_list = [d.page_content for d in relevant_document_chunks]\n",
    "\n",
    "    # Combine document chunks into a single context\n",
    "    context_for_query = \". \".join(context_list)\n",
    "\n",
    "    user_message = qna_user_message_template.replace('{context}', context_for_query)\n",
    "    user_message = user_message.replace('{question}', user_input)\n",
    "\n",
    "    prompt = qna_system_message + '\\n' + user_message\n",
    "\n",
    "    #print(\"Prompt for LLM:\\n\", prompt)\n",
    "\n",
    "    # Generate the response\n",
    "    try:\n",
    "        response = lcpp_llm(\n",
    "                  prompt=prompt,\n",
    "                  max_tokens=max_tokens,\n",
    "                  temperature=temperature,\n",
    "                  top_p=top_p,\n",
    "                  top_k=top_k\n",
    "                  )\n",
    "\n",
    "        # Extract and print the model's response\n",
    "        response = response['choices'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        response = f'Sorry, I encountered the following error: \\n {e}'\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffP1SRYbPQHN"
   },
   "source": [
    "## Question Answering using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjajBEj06B0E"
   },
   "source": [
    "### Query 1: What is the protocol for managing sepsis in a critical care unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Nlo9sMpPRbTP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG:\n",
      "1. Provide supplemental oxygen and secure airway if necessary through intubation and mechanical ventilation.\n",
      "2. Insert two large IV catheters into separate peripheral veins or use a central venous line or intraosseous needle for access.\n",
      "3. Initiate aggressive fluid resuscitation.\n",
      "4. Administer antibiotics promptly based on culture results and suspected organism, and consider surgical intervention for infected or necrotic tissues.\n"
     ]
    }
   ],
   "source": [
    "response_text_rag1 = generate_rag_response(query1,k=3,max_tokens=256,temperature=0,top_p=0.95,top_k=50)\n",
    "print(\"Response with RAG:\")\n",
    "print(response_text_rag1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDw8zXuq6B0F"
   },
   "source": [
    "### Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "PVReF4G8RbzR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG:\n",
      "1. Common symptoms include epigastric or periumbilical pain followed by brief nausea, vomiting, anorexia, and a shift in pain to the right lower quadrant after a few hours. Pain increases with cough and motion. Direct and rebound tenderness are located at McBurney's point.\n",
      "2. Appendicitis cannot be cured via medicine; it requires surgical removal.\n",
      "3. The standard surgical procedure for appendicitis is an appendectomy, which involves removing the inflamed appendix.\n",
      "4. Before the operation, an NGT is inserted, and patients with signs of volume depletion should have urine output monitored with a catheter. IV antibiotics effective against intestinal flora are given to maintain fluid status and prevent infection.\n"
     ]
    }
   ],
   "source": [
    "response_text_rag2 = generate_rag_response(query2,k=3,max_tokens=256,temperature=0,top_p=0.95,top_k=50)\n",
    "print(\"Response with RAG:\")\n",
    "print(response_text_rag2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TggYyQPL6B0G"
   },
   "source": [
    "### Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "0aRbadGtRcX0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG:\n",
      "1. Sudden patchy hair loss can be caused by various conditions such as fungal infections (tinea capitis), bacterial infections, or autoimmune disorders like alopecia areata.\n",
      "2. For fungal infections, topical or oral antifungals are effective treatments.\n",
      "3. For alopecia areata, treatment options include topical corticosteroids, minoxidil, anthralin, immunotherapy (diphencyprone or squaric acid dibutylester), or psoralen plus ultraviolet A (PUVA).\n",
      "4. In cases of trichotillomania, behavior modification, clomipramine, or selective serotonin reuptake inhibitors (SSRIs) may be beneficial.\n",
      "\n",
      "Note: The provided context does not mention any surgical options for treating sudden patchy hair loss specifically.\n"
     ]
    }
   ],
   "source": [
    "response_text_rag3 = generate_rag_response(query3,k=3,max_tokens=256,temperature=0,top_p=0.95,top_k=50)\n",
    "print(\"Response with RAG:\")\n",
    "print(response_text_rag3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TgxdI-_6B0G"
   },
   "source": [
    "### Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "0vzRX1TcRc29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG:\n",
      "1. Ensure reliable airway and maintain adequate ventilation, oxygenation, and blood pressure.\n",
      "2. Surgery may be needed for monitoring intracranial pressure, decompression, or hematoma removal.\n",
      "3. In the first few days, maintain adequate brain perfusion and oxygenation to prevent complications.\n",
      "4. Subsequently, rehabilitation is often required to improve functional recovery.\n"
     ]
    }
   ],
   "source": [
    "response_text_rag4 = generate_rag_response(query4,k=3,max_tokens=256,temperature=0,top_p=0.95,top_k=50)\n",
    "print(\"Response with RAG:\")\n",
    "print(response_text_rag4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlHXYCkm6B0H"
   },
   "source": [
    "### Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "sarpUibcRdhq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG:\n",
      "1. Treat life-threatening injuries first, such as excessive bleeding or signs of shock.\n",
      "2. Immobilize the injured leg using a splint to prevent further damage.\n",
      "3. For lower-limb prosthesis users, maintain body alignment and adjust fit as needed to prevent skin breakdown.\n",
      "4. Seek medical attention for hip fracture surgery and rehabilitation, including walking aids or orthotics for recovery.\n"
     ]
    }
   ],
   "source": [
    "response_text_rag5 = generate_rag_response(query5,k=3,max_tokens=256,temperature=0,top_p=0.95,top_k=50)\n",
    "print(\"Response with RAG:\")\n",
    "print(response_text_rag5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7TYrqycEITB"
   },
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "7UYBR-hcReSo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG FT:\n",
      "1. Provide supplemental oxygen and secure airway if necessary through intubation and mechanical ventilation.\n",
      "2. Insert two large IV catheters into separate peripheral veins or use a central venous line or intraosseous needle for access.\n",
      "3. Initiate aggressive fluid resuscitation.\n",
      "4. Administer antibiotics promptly based on culture results and suspected organism, and consider surgical intervention for infected or necrotic tissues.\n"
     ]
    }
   ],
   "source": [
    "response_text_rag1_ft = generate_rag_response(query1,max_tokens=256, top_k=20) ## moving top_k to 20 to make it deterministic\n",
    "print(\"Response with RAG FT:\")\n",
    "print(response_text_rag1_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG FT:\n",
      "1. Common symptoms include epigastric or periumbilical pain followed by brief nausea, vomiting, anorexia, and a shift in pain to the right lower quadrant after a few hours. Pain increases with cough and motion. Direct and rebound tenderness are located at McBurney's point.\n",
      "2. Appendicitis cannot be cured via medicine; it requires surgical removal.\n",
      "3. The standard surgical procedure for appendicitis is an appendectomy, which involves removing the inflamed appendix.\n",
      "4. Before the operation, an NGT is inserted, and patients with signs of volume depletion should have urine output monitored with a catheter. IV antibiotics effective against intestinal flora are given to maintain fluid status and prevent infection.\n"
     ]
    }
   ],
   "source": [
    "response_text_rag2_ft = generate_rag_response(query2,max_tokens= 256, top_k=20) ## moving top_k to 20 to make it deterministic\n",
    "print(\"Response with RAG FT:\")\n",
    "print(response_text_rag2_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG FT:\n",
      "1. Sudden patchy hair loss can be caused by various conditions such as fungal infections (tinea capitis), bacterial infections, or autoimmune disorders like alopecia areata.\n",
      "2. For fungal infections, topical or oral antifungals are effective treatments.\n",
      "3. For alopecia areata, treatment options include topical corticosteroids, minoxidil, anthralin, immunotherapy (diphencyprone or squaric acid dibutylester), or psoralen plus ultraviolet A (PUVA).\n",
      "4. In cases of trichotillomania, behavior modification, clomipramine, or selective serotonin reuptake inhibitors (SSRIs) may be beneficial.\n",
      "\n",
      "Note: The provided context does not mention any surgical options for treating sudden patchy hair loss specifically.\n"
     ]
    }
   ],
   "source": [
    "response_text_rag3_ft = generate_rag_response(query3,max_tokens=256,top_k=20) ## moving top_k to 20 to make it deterministic\n",
    "print(\"Response with RAG FT:\")\n",
    "print(response_text_rag3_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG FT:\n",
      "1. Ensure reliable airway and maintain adequate ventilation, oxygenation, and blood pressure.\n",
      "2. Surgery may be needed for monitoring intracranial pressure, decompression, or hematoma removal.\n",
      "3. In the first few days, maintain adequate brain perfusion and oxygenation to prevent complications.\n",
      "4. Subsequently, rehabilitation is often required to improve functional recovery.\n"
     ]
    }
   ],
   "source": [
    "response_text_rag4_ft = generate_rag_response(query4,max_tokens=256,top_k=20) ## moving top_k to 20 to make it deterministic\n",
    "print(\"Response with RAG FT:\")\n",
    "print(response_text_rag4_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG FT:\n",
      "1. Treat life-threatening injuries first, such as excessive bleeding or signs of shock.\n",
      "2. Immobilize the injured leg using a splint to prevent further damage.\n",
      "3. For lower-limb prosthesis users, maintain body alignment and adjust fit as needed to prevent skin breakdown.\n",
      "4. Seek medical attention for hip fracture surgery and rehabilitation, including walking aids or orthotics for recovery.\n"
     ]
    }
   ],
   "source": [
    "response_text_rag5_ft = generate_rag_response(query5,max_tokens=256,top_k=20) ## moving top_k to 20 to make it deterministic\n",
    "print(\"Response with RAG FT:\")\n",
    "print(response_text_rag5_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyQrTipNfuBN"
   },
   "source": [
    "## Output Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "IHbfLAxAGdhW"
   },
   "outputs": [],
   "source": [
    "groundedness_rater_system_message = (\n",
    "    \"You are a medical expert evaluating the groundedness of an AI-generated medical answer. \"\n",
    "    \"Your task is to assess how well the answer is strictly supported by the provided context from the Merck Manual PDF. \"\n",
    "    \"Rate the answer based on these criteria:\\n\"\n",
    "    \"- 'Fully Grounded': All medical information comes directly from the provided context, with accurate facts and procedures\\n\"\n",
    "    \"- 'Partially Grounded': Some information is supported by the context, but contains unsupported details or generalizations\\n\"\n",
    "    \"- 'Not Grounded': Information is not found in the context or contradicts the provided medical manual content\\n\\n\"\n",
    "    \"IMPORTANT: Medical answers must be factually accurate and traceable to the source material. \"\n",
    "    \"Any medical advice, treatments, or procedures mentioned must be explicitly found in the Merck Manual context. \"\n",
    "    \"Respond with your rating and provide a detailed justification in 2-3 sentences explaining your assessment.\\n\\n\"\n",
    "    \"Sample answer format:\\n\"\n",
    "    \"Rating: Fully Grounded\\n\"\n",
    "    \"Justification: The answer accurately reflects the sepsis management protocols outlined in the Merck Manual context, \"\n",
    "    \"including specific treatment steps and medication recommendations that are directly referenced in the provided text.\"\n",
    ")\n",
    "\n",
    "relevance_rater_system_message = (\n",
    "    \"You are a medical expert evaluating the relevance of an AI-generated answer. \"\n",
    "    \"Given the user's question and the answer, rate how relevant the answer is to the question. \"\n",
    "    \"Respond with one of: 'Highly Relevant', 'Somewhat Relevant', or 'Not Relevant'. \"\n",
    "    \"And in the next line, briefly justify your rating in 1-2 sentences.\"\n",
    "    \"Sample answer: Rating: Higly Relevant \\\n",
    "    Justification:  The answer directly addresses the user's question, providing specific and actionable information.\"\n",
    ")\n",
    "\n",
    "user_message_template = (\n",
    "    \"Context:\\n{context}\\n\\n\"\n",
    "    \"Answer:\\n{answer}\\n\\n\"\n",
    "    \"Question:\\n{question}\\n\\n\"\n",
    "    \"Please provide your rating and justification.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "XIbZybyuRi2p"
   },
   "outputs": [],
   "source": [
    "def generate_ground_relevance_response(user_input,k=3,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
    "    global qna_system_message,qna_user_message_template\n",
    "    # Retrieve relevant document chunks\n",
    "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=3)\n",
    "    pages = [str(doc.metadata.get('page', 'N/A')) for doc in relevant_document_chunks]\n",
    "    pages_str = \", \".join(pages)\n",
    "\n",
    "    context_list = [d.page_content for d in relevant_document_chunks]\n",
    "    context_for_query = \". \".join(context_list)\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    prompt = f\"\"\"[INST]{qna_system_message}\\n\n",
    "                {'user'}: {qna_user_message_template.format(context=context_for_query, question=user_input)}\n",
    "                [/INST]\"\"\"\n",
    "\n",
    "    response = lcpp_llm(\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            stop=['INST'],\n",
    "            echo=False\n",
    "            )\n",
    "\n",
    "    answer =  response[\"choices\"][0][\"text\"]\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    groundedness_prompt = f\"\"\"[INST]{groundedness_rater_system_message}\\n\n",
    "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n",
    "                [/INST]\"\"\"\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    relevance_prompt = f\"\"\"[INST]{relevance_rater_system_message}\\n\n",
    "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n",
    "                [/INST]\"\"\"\n",
    "\n",
    "    response_1 =  lcpp_llm(\n",
    "            prompt=groundedness_prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            stop=['INST'],\n",
    "            echo=False\n",
    "            )\n",
    "\n",
    "    response_2 = lcpp_llm(\n",
    "            prompt=relevance_prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            stop=['INST'],\n",
    "            echo=False\n",
    "            )\n",
    "\n",
    "    return response_1['choices'][0]['text'],response_2['choices'][0]['text'],pages_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the protocol for managing sepsis in a ...</td>\n",
       "      <td>1. Provide supplemental oxygen and secure airw...</td>\n",
       "      <td>Rating: Fully Grounded\\nJustification: The ans...</td>\n",
       "      <td>Rating: Highly Relevant\\nJustification: The an...</td>\n",
       "      <td>2400, 2448, 2453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the common symptoms for appendicitis,...</td>\n",
       "      <td>1. Common symptoms include epigastric or periu...</td>\n",
       "      <td>Rating: Fully Grounded\\nJustification: The ans...</td>\n",
       "      <td>Rating: Highly Relevant\\nJustification: The an...</td>\n",
       "      <td>173, 172, 3567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the effective treatments or solutions...</td>\n",
       "      <td>1. Sudden patchy hair loss can be caused by va...</td>\n",
       "      <td>Rating: Fully Grounded\\nJustification: The ans...</td>\n",
       "      <td>Rating: Highly Relevant\\nJustification: The an...</td>\n",
       "      <td>858, 855, 857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What treatments are recommended for a person w...</td>\n",
       "      <td>1. Ensure reliable airway and maintain adequat...</td>\n",
       "      <td>Rating: Fully Grounded\\nJustification: The ans...</td>\n",
       "      <td>Rating: Highly Relevant\\nJustification: The an...</td>\n",
       "      <td>3647, 3409, 3404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the necessary precautions and treatme...</td>\n",
       "      <td>1. Treat life-threatening injuries first, such...</td>\n",
       "      <td>Rating: Fully Grounded\\nJustification: The ans...</td>\n",
       "      <td>Rating: Highly Relevant\\nJustification: The an...</td>\n",
       "      <td>3390, 3646, 1023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0  What is the protocol for managing sepsis in a ...   \n",
       "1  What are the common symptoms for appendicitis,...   \n",
       "2  What are the effective treatments or solutions...   \n",
       "3  What treatments are recommended for a person w...   \n",
       "4  What are the necessary precautions and treatme...   \n",
       "\n",
       "                                              Answer  \\\n",
       "0  1. Provide supplemental oxygen and secure airw...   \n",
       "1  1. Common symptoms include epigastric or periu...   \n",
       "2  1. Sudden patchy hair loss can be caused by va...   \n",
       "3  1. Ensure reliable airway and maintain adequat...   \n",
       "4  1. Treat life-threatening injuries first, such...   \n",
       "\n",
       "                                        Groundedness  \\\n",
       "0  Rating: Fully Grounded\\nJustification: The ans...   \n",
       "1  Rating: Fully Grounded\\nJustification: The ans...   \n",
       "2  Rating: Fully Grounded\\nJustification: The ans...   \n",
       "3  Rating: Fully Grounded\\nJustification: The ans...   \n",
       "4  Rating: Fully Grounded\\nJustification: The ans...   \n",
       "\n",
       "                                           Relevance             Pages  \n",
       "0  Rating: Highly Relevant\\nJustification: The an...  2400, 2448, 2453  \n",
       "1  Rating: Highly Relevant\\nJustification: The an...    173, 172, 3567  \n",
       "2  Rating: Highly Relevant\\nJustification: The an...     858, 855, 857  \n",
       "3  Rating: Highly Relevant\\nJustification: The an...  3647, 3409, 3404  \n",
       "4  Rating: Highly Relevant\\nJustification: The an...  3390, 3646, 1023  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for q, ans in [\n",
    "    (query1, response_text_rag1_ft),\n",
    "    (query2, response_text_rag2_ft),\n",
    "    (query3, response_text_rag3_ft),\n",
    "    (query4, response_text_rag4_ft),\n",
    "    (query5, response_text_rag5_ft),\n",
    "]:\n",
    "    ground, rel, page_str = generate_ground_relevance_response(user_input=q, max_tokens=370, top_k=20)\n",
    "    results.append({\n",
    "        \"Query\": q,\n",
    "        \"Answer\": ans,\n",
    "        \"Groundedness\": ground.strip(),\n",
    "        \"Relevance\": rel.strip(),\n",
    "        \"Pages\": page_str if \"Not Grounded\" not in ground else None\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(results)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query 1:** What is the protocol for managing sepsis in a critical care unit?\n",
       "\n",
       "**Answer:**\n",
       "1. Provide supplemental oxygen and secure airway if necessary through intubation and mechanical ventilation.\n",
       "2. Insert two large IV catheters into separate peripheral veins or use a central venous line or intraosseous needle for access.\n",
       "3. Initiate aggressive fluid resuscitation.\n",
       "4. Administer antibiotics promptly based on culture results and suspected organism, and consider surgical intervention for infected or necrotic tissues.\n",
       "\n",
       "**Groundedness:** Rating: Fully Grounded\n",
       "Justification: The answer accurately reflects the sepsis management protocols outlined in the Merck Manual context, including providing supplemental oxygen, checking airway and ventilation, inserting large IV catheters for fluid resuscitation, initiating aggressive fluid resuscitation, antibiotics, and considering surgical intervention if necessary. These steps are directly referenced in the provided text.\n",
       "\n",
       "**Pages:** 2400, 2448, 2453\n",
       "\n",
       "**Relevance:** Rating: Highly Relevant\n",
       "Justification: The answer directly addresses the user's question by providing specific steps for managing sepsis in a critical care unit, including supplemental oxygen, checking airway and ventilation, inserting large IV catheters for fluid resuscitation, aggressive fluid resuscitation, antibiotics, surgical intervention if necessary, and supportive care.\n",
       "\n",
       "---\n",
       "**Query 2:** What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n",
       "\n",
       "**Answer:**\n",
       "1. Common symptoms include epigastric or periumbilical pain followed by brief nausea, vomiting, anorexia, and a shift in pain to the right lower quadrant after a few hours. Pain increases with cough and motion. Direct and rebound tenderness are located at McBurney's point.\n",
       "2. Appendicitis cannot be cured via medicine; it requires surgical removal.\n",
       "3. The standard surgical procedure for appendicitis is an appendectomy, which involves removing the inflamed appendix.\n",
       "4. Before the operation, an NGT is inserted, and patients with signs of volume depletion should have urine output monitored with a catheter. IV antibiotics effective against intestinal flora are given to maintain fluid status and prevent infection.\n",
       "\n",
       "**Groundedness:** Rating: Fully Grounded\n",
       "Justification: The answer accurately reflects the symptoms of appendicitis as described in the Merck Manual context, including epigastric or periumbilical pain, nausea, vomiting, anorexia, and right lower quadrant pain that increases with cough and motion. The answer also correctly states that appendicitis cannot be cured via medicine and requires surgical removal, specifically an appendectomy, which is also mentioned in the Merck Manual context. Before surgery, patients may receive an NGT, urinary catheter, IV fluids and electrolytes, and antibiotics effective against intestinal flora as stated in the answer, which is also supported by the Merck Manual context.\n",
       "\n",
       "**Pages:** 173, 172, 3567\n",
       "\n",
       "**Relevance:** Rating: Highly Relevant\n",
       "Justification: The answer directly addresses the user's question by providing a clear list of common symptoms for appendicitis and stating that it cannot be cured via medicine, but rather requires surgical removal through an appendectomy. The answer also mentions the pre-surgery preparations such as NGT, urinary catheter, IV fluids and electrolytes, and antibiotics.\n",
       "\n",
       "---\n",
       "**Query 3:** What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n",
       "\n",
       "**Answer:**\n",
       "1. Sudden patchy hair loss can be caused by various conditions such as fungal infections (tinea capitis), bacterial infections, or autoimmune disorders like alopecia areata.\n",
       "2. For fungal infections, topical or oral antifungals are effective treatments.\n",
       "3. For alopecia areata, treatment options include topical corticosteroids, minoxidil, anthralin, immunotherapy (diphencyprone or squaric acid dibutylester), or psoralen plus ultraviolet A (PUVA).\n",
       "4. In cases of trichotillomania, behavior modification, clomipramine, or selective serotonin reuptake inhibitors (SSRIs) may be beneficial.\n",
       "\n",
       "Note: The provided context does not mention any surgical options for treating sudden patchy hair loss specifically.\n",
       "\n",
       "**Groundedness:** Rating: Fully Grounded\n",
       "Justification: The answer accurately reflects the information provided in the Merck Manual context regarding the causes (alopecia areata, tinea capitis, trichotillomania) and treatments (topical corticosteroids, minoxidil, anthralin, immunotherapy, systemic corticosteroids, antifungals, behavior modification techniques, clomipramine, and SSRIs) for sudden patchy hair loss. The answer also includes additional details about alopecia areata being an autoimmune disorder that may progress to permanent baldness if left untreated, and the use of fungal and bacterial cultures and immunofluorescence studies for diagnosis.\n",
       "\n",
       "**Pages:** 858, 855, 857\n",
       "\n",
       "**Relevance:** Rating: Highly Relevant\n",
       "Justification: The answer directly addresses the user's question by providing information about the potential causes of sudden patchy hair loss (alopecia areata, tinea capitis, trichotillomania) and effective treatments for each condition.\n",
       "\n",
       "---\n",
       "**Query 4:** What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n",
       "\n",
       "**Answer:**\n",
       "1. Ensure reliable airway and maintain adequate ventilation, oxygenation, and blood pressure.\n",
       "2. Surgery may be needed for monitoring intracranial pressure, decompression, or hematoma removal.\n",
       "3. In the first few days, maintain adequate brain perfusion and oxygenation to prevent complications.\n",
       "4. Subsequently, rehabilitation is often required to improve functional recovery.\n",
       "\n",
       "**Groundedness:** Rating: Fully Grounded\n",
       "Justification: The answer accurately reflects the treatment recommendations for traumatic brain injury (TBI) as outlined in the Merck Manual context. The answer includes specific treatments such as ensuring a reliable airway, maintaining adequate ventilation, oxygenation, and blood pressure, and the potential need for surgery to place monitors, decompress the brain, or remove intracranial hematomas. Additionally, the importance of rehabilitation for functional recovery and preventing secondary disabilities is mentioned, which aligns with the information provided in the context on pages 3231 and 3400.\n",
       "\n",
       "**Pages:** 3647, 3409, 3404\n",
       "\n",
       "**Relevance:** Rating: Highly Relevant\n",
       "Justification: The answer directly addresses the user's question by providing specific treatments recommended for a person with a traumatic brain injury, including ensuring a reliable airway, maintaining adequate ventilation, oxygenation, and blood pressure, surgery if necessary, and rehabilitation.\n",
       "\n",
       "---\n",
       "**Query 5:** What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n",
       "\n",
       "**Answer:**\n",
       "1. Treat life-threatening injuries first, such as excessive bleeding or signs of shock.\n",
       "2. Immobilize the injured leg using a splint to prevent further damage.\n",
       "3. For lower-limb prosthesis users, maintain body alignment and adjust fit as needed to prevent skin breakdown.\n",
       "4. Seek medical attention for hip fracture surgery and rehabilitation, including walking aids or orthotics for recovery.\n",
       "\n",
       "**Groundedness:** Rating: Fully Grounded\n",
       "Justification: The answer accurately reflects the recommended precautions and treatment steps for a person with a fractured leg as outlined in the Merck Manual context, including prioritizing life-threatening injuries, immobilization using a splint, seeking medical attention for definitive treatment, applying RICE for comfort and swelling reduction, and maintaining proper body alignment during recovery for individuals with lower-limb prostheses. The answer also emphasizes the importance of following healthcare practitioner instructions during hip fracture surgery rehabilitation. All information is directly referenced in the provided text.\n",
       "\n",
       "**Pages:** 3390, 3646, 1023\n",
       "\n",
       "**Relevance:** Rating: Highly Relevant\n",
       "Justification: The answer directly addresses the user's question by providing specific and actionable information on the necessary precautions and treatment steps for a person who has fractured their leg, as well as considerations for their care and recovery.\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Create a more readable summary of the evaluation results, including pages\n",
    "\n",
    "def format_eval_content(df):\n",
    "    content = \"\"\n",
    "    for idx, row in df.iterrows():\n",
    "        content += f\"**Query {idx+1}:** {row['Query']}\\n\\n\"\n",
    "        content += f\"**Answer:**\\n{row['Answer']}\\n\\n\"\n",
    "        content += f\"**Groundedness:** {row['Groundedness']}\\n\\n\"\n",
    "        if 'Pages' in row and pd.notnull(row['Pages']):\n",
    "            content += f\"**Pages:** {row['Pages']}\\n\\n\"\n",
    "        content += f\"**Relevance:** {row['Relevance']}\\n\\n\"\n",
    "        content += \"---\\n\"\n",
    "    return content\n",
    "\n",
    "readable_eval_content = format_eval_content(eval_df)\n",
    "display(Markdown(readable_eval_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actionable Insights and Business Recommendations\n",
    "\n",
    "\n",
    "1. **Improved Contextual Responses**:\n",
    "    - RAG-based approach enhances relevance and groundedness using trusted sources like the Merck Manual.\n",
    "    - Fine-tuned parameters (`temperature=0`, `top_p=0.95`, `top_k=20`) ensure consistent outputs.\n",
    "\n",
    "2. **Efficient Data Handling**:\n",
    "    - Chunking with RecursiveCharacterTextSplitter preserves context, creating 18,088 chunks from 4,114 pages.\n",
    "    - Chroma vector database enables fast, domain-specific retrieval.\n",
    "\n",
    "3. **Evaluation Metrics**:\n",
    "    - Groundedness: 100% Fully Grounded.\n",
    "    - Relevance: 100% Highly Relevant.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Business Recommendations:\n",
    "1. **Healthcare Assistant Product**:\n",
    "    - Develop an AI-powered assistant for clinicians and patients.\n",
    "\n",
    "2. **Expand Knowledge Base**:\n",
    "    - Add sources like PubMed to cover broader medical domains.\n",
    "\n",
    "3. **Regulatory Compliance**:\n",
    "    - Ensure HIPAA/GDPR compliance for data security.\n",
    "\n",
    "4. **Specialized Solutions**:\n",
    "    - Tailor systems for specialties like neurology or cardiology.\n",
    "\n",
    "5. **Continuous Updates**:\n",
    "    - Regularly update the vector database with new medical research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybRlzaIhWaM9"
   },
   "source": [
    "<font size=6 color='blue'>Power Ahead</font>\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3CNz35ia6Bz3",
    "CkRbhMJH6Bz3",
    "CARPKFwm6Bz4",
    "by9EvAnkSpZf",
    "lnwETBOE6Bz5",
    "TtZWqj0wFTS1",
    "Uq1lhM4WFTS2",
    "EzzkvIXvFTS4",
    "K8YgK91SFjVY",
    "J6yxICeVFjVc",
    "oflaoOGiFjVd",
    "WUUqY4FbFjVe",
    "5laPFTHrFjVf",
    "g5myZ5dOOefc",
    "9Jg3r_LWOeff",
    "iYpyw4HjOeff",
    "dRp92JQZOeff",
    "AA45zwyUOefg",
    "TYXxiSuBOefg",
    "t_O1PGdNO2M9",
    "uTpWESc53dL9",
    "ffj0ca3eZT4u",
    "f9weTDzMxRRS",
    "7-wNNalNxPKT",
    "LECMxTH-zB-R",
    "BvHVejcWz0Bl",
    "qiKCOv4X0d7B",
    "uEa5sKc41T1z",
    "vw8qcwq66B0C",
    "TkIteX4m6mny",
    "ffP1SRYbPQHN",
    "JjajBEj06B0E",
    "QDw8zXuq6B0F",
    "TggYyQPL6B0G",
    "1TgxdI-_6B0G",
    "FlHXYCkm6B0H",
    "K7TYrqycEITB",
    "Y7QICRU-njdj"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
