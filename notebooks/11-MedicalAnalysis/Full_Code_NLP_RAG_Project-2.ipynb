{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The healthcare industry is rapidly evolving, with professionals facing increasing challenges in managing vast volumes of medical data while delivering accurate and timely diagnoses. The need for quick access to comprehensive, reliable, and up-to-date medical knowledge is critical for improving patient outcomes and ensuring informed decision-making in a fast-paced environment.\n",
    "\n",
    "Healthcare professionals often encounter information overload, struggling to sift through extensive research and data to create accurate diagnoses and treatment plans. This challenge is amplified by the need for efficiency, particularly in emergencies, where time-sensitive decisions are vital. Furthermore, access to trusted, current medical information from renowned manuals and research papers is essential for maintaining high standards of care.\n",
    "\n",
    "To address these challenges, healthcare centers can focus on integrating systems that streamline access to medical knowledge, provide tools to support quick decision-making, and enhance efficiency. Leveraging centralized knowledge platforms and ensuring healthcare providers have continuous access to reliable resources can significantly improve patient care and operational effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Questions to Answer**\n",
    "\n",
    "**1. Diagnostic Assistance**: \"What are the common symptoms and treatments for pulmonary embolism?\"\n",
    "\n",
    "**2. Drug Information**: \"Can you provide the trade names of medications used for treating hypertension?\"\n",
    "\n",
    "**3. Treatment Plans**: \"What are the first-line options and alternatives for managing rheumatoid arthritis?\"\n",
    "\n",
    "**4. Specialty Knowledge**: \"What are the diagnostic steps for suspected endocrine disorders?\"\n",
    "\n",
    "**5. Critical Care Protocols**: \"What is the protocol for managing sepsis in a critical care unit?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an AI specialist, your task is to develop a RAG-based AI solution using renowned medical manuals to address healthcare challenges. The objective is to **understand** issues like information overload, **apply** AI techniques to streamline decision-making, **analyze** its impact on diagnostics and patient outcomes, **evaluate** its potential to standardize care practices, and **create** a functional prototype demonstrating its feasibility and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Merck Manuals** are medical references published by the American pharmaceutical company Merck & Co., that cover a wide range of medical topics, including disorders, tests, diagnoses, and drugs. The manuals have been published since 1899, when Merck & Co. was still a subsidiary of the German company Merck.\n",
    "\n",
    "The manual is provided as a PDF with over 4,000 pages divided into 23 sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnwETBOE6Bz5"
   },
   "source": [
    "## Installing and Importing Necessary Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4GgLhZhUM4V",
    "outputId": "5199a58a-d12e-4f69-dd31-4de62a8c1bfa"
   },
   "outputs": [],
   "source": [
    "# Installation for GPU llama-cpp-python\n",
    "# uncomment and run the following code in case GPU is being used\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=OFF\" FORCE_CMAKE=1 #pip install llama-cpp-python --force-reinstall --no-cache-dir -q\n",
    "\n",
    "# Installation for CPU llama-cpp-python\n",
    "# uncomment and run the following code in case GPU is not being used\n",
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=off\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0VOckDVkWGei",
    "outputId": "0ff6fa24-34cd-4dba-9a8c-1d8e8e3fda35"
   },
   "outputs": [],
   "source": [
    "# For installing the libraries & downloading models from HF Hub\n",
    "#!pip install huggingface_hub==0.23.2 pandas==1.5.3 tiktoken==0.6.0 pymupdf==1.25.1 langchain==0.1.1 langchain-community==0.0.13 chromadb==0.4.22 sentence-transformers==2.3.1 numpy==1.25.2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RTY9GN4oWK3g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jithinravi/work/python/ut_ai_ml/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Libraries for processing dataframes,text\n",
    "import json,os\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "\n",
    "#Libraries for Loading Data, Chunking, Embedding, and Vector Databases\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#Libraries for downloading and loading the llm\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtZWqj0wFTS1"
   },
   "source": [
    "## Question Answering using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq1lhM4WFTS2"
   },
   "source": [
    "#### Downloading and Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
    "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.8 (v3.12.8:2dc476bcb91, Dec  3 2024, 14:43:19) [Clang 13.0.0 (clang-1300.0.29.30)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dA3XQMWmQLJp"
   },
   "outputs": [],
   "source": [
    "# Using hf_hub_download to download a model from the Hugging Face model hub\n",
    "# The repo_id parameter specifies the model name or path in the Hugging Face repository\n",
    "# The filename parameter specifies the name of the file to download\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=model_name_or_path,\n",
    "    filename=model_basename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/jithinravi/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q6_K.gguf (version unknown)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:              blk.8.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:              blk.9.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.10.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.11.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:             blk.12.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.17.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:             blk.26.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:             blk.31.ffn_up.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32     \n",
      "llama_model_loader: - kv  11:                          general.file_type u32     \n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32     \n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool    \n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool    \n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str     \n",
      "llama_model_loader: - kv  23:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_print_meta: format         = unknown\n",
      "llm_load_print_meta: arch           = llama\n",
      "llm_load_print_meta: vocab type     = SPM\n",
      "llm_load_print_meta: n_vocab        = 32000\n",
      "llm_load_print_meta: n_merges       = 0\n",
      "llm_load_print_meta: n_ctx_train    = 32768\n",
      "llm_load_print_meta: n_ctx          = 4096\n",
      "llm_load_print_meta: n_embd         = 4096\n",
      "llm_load_print_meta: n_head         = 32\n",
      "llm_load_print_meta: n_head_kv      = 8\n",
      "llm_load_print_meta: n_layer        = 32\n",
      "llm_load_print_meta: n_rot          = 128\n",
      "llm_load_print_meta: n_gqa          = 4\n",
      "llm_load_print_meta: f_norm_eps     = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps = 1.0e-05\n",
      "llm_load_print_meta: n_ff           = 14336\n",
      "llm_load_print_meta: freq_base      = 1000000.0\n",
      "llm_load_print_meta: freq_scale     = 1\n",
      "llm_load_print_meta: model type     = 7B\n",
      "llm_load_print_meta: model ftype    = mostly Q6_K\n",
      "llm_load_print_meta: model size     = 7.24 B\n",
      "llm_load_print_meta: general.name   = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: mem required  = 5666.19 MB (+  512.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: kv self size  =  512.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4\n",
      "ggml_metal_init: picking default device: Apple M4\n",
      "ggml_metal_init: loading '/Users/jithinravi/work/python/ut_ai_ml/.venv/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x137cf0620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                        0x137cf55d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                            0x137cf5dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x137cf7f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                          0x137cf85b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                           0x137cf7a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                           0x137cf9a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                           0x137cfa540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x137cfb410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_4                     0x137cfbc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x137cfc490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                0x137cfaad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x137cfd710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x137cfdf60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x137cfe7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                  0x137cff000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x137cff850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x147e04360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x147e04990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x147e05360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x147e05b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x136dae290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                           0x136daf430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x147e062e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32_1row           0x147e07420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32_l4             0x147e06900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x147e08090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x147e08ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q8_0_f32               0x147e09780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x147e0a070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x147e0a980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x147e0b350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x147e0c2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x136daf9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x136db00f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x136daee50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                0x136db0520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x136db1690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x136db1f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x136db27f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x147e0c5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x147e0cb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x147e0d9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope                           0x147e0e350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x147e0eb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x147e0e6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x147e10ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x147e10130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 10922.67 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =  285.47 MB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  5666.80 MB, ( 5667.50 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.48 MB, ( 5668.98 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   514.00 MB, ( 6182.98 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   284.02 MB, ( 6467.00 / 10922.67)\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: kv self size  =  512.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4\n",
      "ggml_metal_init: picking default device: Apple M4\n",
      "ggml_metal_init: loading '/Users/jithinravi/work/python/ut_ai_ml/.venv/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x137cf0620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                        0x137cf55d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                            0x137cf5dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x137cf7f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                          0x137cf85b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                           0x137cf7a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                           0x137cf9a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                           0x137cfa540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x137cfb410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_4                     0x137cfbc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x137cfc490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                0x137cfaad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x137cfd710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x137cfdf60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x137cfe7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                  0x137cff000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x137cff850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x147e04360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x147e04990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x147e05360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x147e05b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x136dae290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                           0x136daf430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x147e062e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32_1row           0x147e07420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32_l4             0x147e06900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x147e08090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x147e08ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q8_0_f32               0x147e09780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x147e0a070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x147e0a980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x147e0b350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x147e0c2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x136daf9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x136db00f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x136daee50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                0x136db0520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x136db1690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x136db1f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x136db27f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x147e0c5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x147e0cb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x147e0d9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope                           0x147e0e350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x147e0eb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x147e0e6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x147e10ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x147e10130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 10922.67 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =  285.47 MB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  5666.80 MB, ( 5667.50 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.48 MB, ( 5668.98 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   514.00 MB, ( 6182.98 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   284.02 MB, ( 6467.00 / 10922.67)\n"
     ]
    }
   ],
   "source": [
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=10,  # CPU cores\n",
    "    n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=5,  # uncomment and change this value based on GPU VRAM pool.\n",
    "    n_ctx=4096,  # Context window\n",
    "    verbose=False,  # Reduce verbose output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzzkvIXvFTS4"
   },
   "source": [
    "#### Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hG_IaZj0QLw4"
   },
   "outputs": [],
   "source": [
    "def response(query,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
    "    model_output = lcpp_llm (\n",
    "      prompt=query,\n",
    "      max_tokens=max_tokens,\n",
    "      temperature=temperature,\n",
    "      top_p=top_p,\n",
    "      top_k=top_k\n",
    "    )\n",
    "    #print(\"Model Output:\", model_output)\n",
    "    return model_output['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8YgK91SFjVY"
   },
   "source": [
    "### Query 1: What is the protocol for managing sepsis in a critical care unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-JLIVmpPQH0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "\n",
      "Sepsis is a life-threatening condition that can arise from an infection, and it requires prompt recognition and aggressive management in a critical care unit. The following are general steps for managing sepsis in a critical care unit:\n",
      "\n",
      "1. Early recognition and suspicion: Septic patients may present with non-specific symptoms such as fever, chills, tachycardia, tachypnea, altered mental status, or lactic acidosis. It is essential to have a high index of suspicion for sepsis in patients with suspected or confirmed infection.\n",
      "2. Early goal-directed\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is the protocol for managing sepsis in a critical care unit?\"\n",
    "response_text1 = response(query1)\n",
    "print(\"Response:\", response_text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6yxICeVFjVc"
   },
   "source": [
    "### Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BdiHRgEqQIP9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "\n",
      "Appendicitis is a medical condition characterized by inflammation of the appendix, a small tube-shaped organ located in the lower right side of the abdomen. The symptoms of appendicitis can vary from person to person, but some common signs include:\n",
      "\n",
      "1. Abdominal pain: The pain is typically located in the lower right quadrant of the abdomen and may start as a mild discomfort that gradually worsens over time. The pain may be constant or intermittent and may be aggravated by movement, deep breathing, or coughing.\n",
      "2. Loss of appetite\n"
     ]
    }
   ],
   "source": [
    "query2 = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\n",
    "response2_text = response(query2)\n",
    "print(\"Response:\", response2_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oflaoOGiFjVd"
   },
   "source": [
    "### Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N-mx9yboQIt-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "\n",
      "Sudden patchy hair loss, also known as alopecia areata, is a common autoimmune disorder that affects the hair follicles. It can result in round or oval bald patches on the scalp, but it can also occur on other parts of the body such as the beard area, eyebrows, and eyelashes.\n",
      "\n",
      "The exact cause of alopecia areata is not known, but it's believed to be related to a problem with the immune system. Some possible triggers for this condition include stress, genetics, viral infections, and certain medications.\n"
     ]
    }
   ],
   "source": [
    "query3 = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\n",
    "response3_text = response(query3)\n",
    "print(\"Response:\", response3_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUUqY4FbFjVe"
   },
   "source": [
    "### Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TEsVMaKaQJzh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "\n",
      "A person who has sustained a physical injury to brain tissue, also known as a traumatic brain injury (TBI), may require various treatments depending on the severity and location of the injury. Here are some common treatments recommended for TBIs:\n",
      "\n",
      "1. Emergency care: The first priority is to ensure the person's airway is clear, they are breathing, and their heart is beating normally. In severe cases, emergency surgery may be required to remove hematomas or other obstructions.\n",
      "2. Medications: Depending on the symptoms, medications may be prescribed to manage conditions such as\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\n",
    "response4_text = response(query4)\n",
    "print(\"Response:\", response4_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5laPFTHrFjVf"
   },
   "source": [
    "### Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VfrlmrP5QKJz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "\n",
      "First and foremost, it is essential to ensure the safety of the injured person and any companions. If possible, try to keep the person as comfortable as possible while preventing further injury or complications. Here are some general steps to follow:\n",
      "\n",
      "1. Assess the severity of the injury: Check for signs of open wounds, swelling, deformity, numbness, or inability to move the leg. If the fracture is open (compound), do not attempt to move the person unless it is necessary for their safety. In this case, call for emergency medical assistance immediately.\n",
      "2.\n"
     ]
    }
   ],
   "source": [
    "query5 = \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\n",
    "response5_text = response(query5)\n",
    "print(\"Response:\", response5_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - First Approach - Plain LLM response\n",
    "\n",
    "- **Initial Approach:**  \n",
    "    The plain LLM model is used for question answering without any prompt engineering or parameter tuning.\n",
    "\n",
    "- **Default Model Behavior:**  \n",
    "    The model responds to queries using its pre-trained knowledge, without any additional guidance or customization.\n",
    "\n",
    "- **Limitations:**  \n",
    "    - While this method is straightforward and quick to implement, it may not yield the most accurate or contextually relevant answers—especially for specialized domains like medicine. Without tuning parameters (e.g., `temperature`, `max_tokens`) or providing system/user prompts, the model's outputs can be generic and may lack the specificity or structure required for clinical use. \n",
    "    - We also see the answers are largely `truncated` which is not ideal.\n",
    "\n",
    "- **Advanced Techniques:**  \n",
    "    More advanced methods, such as prompt engineering or Retrieval-Augmented Generation (RAG), can significantly improve the quality and reliability of responses in such applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5myZ5dOOefc"
   },
   "source": [
    "## Question Answering using LLM with Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of prompt engineering for better LLM responses\n",
    "\n",
    "# Define a system prompt to instruct the LLM to answer as a medical expert using evidence from the Merck Manual\n",
    "qna_system_message = (\n",
    "    \"You are a highly knowledgeable medical assistant. \"\n",
    "    \"Be concise, accurate. If you don't know the answer, say 'I don't know'. \"\n",
    "    \"Not more than 4 points in your response.No more than approximately 30 words in each point. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Jg3r_LWOeff"
   },
   "source": [
    "### Query 1: What is the protocol for managing sepsis in a critical care unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Immediate antibiotic administration based on suspected infection source and sensitivity results.\n",
      "2. Aggressive fluid resuscitation to maintain adequate tissue perfusion.\n",
      "3. Close monitoring of vital signs, lactate levels, and organ function.\n",
      "4. Consideration of vasopressors or mechanical ventilation as needed for hemodynamic instability.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe1 = response(qna_system_message + \" \" + query1, max_tokens=256).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "We added a system prompt to guide the model's responses and used the `max_tokens` parameter to ensure the responses are not truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYpyw4HjOeff"
   },
   "source": [
    "### Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Appendicitis: Sudden onset of abdominal pain, usually localized in lower right quadrant. Nausea, vomiting, loss of appetite, fever, and constipation or diarrhea may occur.\n",
      "2. No cure with medicine: Appendicitis requires surgical removal of the appendix (appendectomy) due to its potential rupture and risk of peritonitis.\n",
      "3. Laparoscopic appendectomy: Commonly performed procedure, using small incisions and a laparoscope for minimally invasive surgery.\n",
      "4. Open appendectomy: Traditional surgical method with a larger incision to directly access the appendix, used in cases where laparoscopy is not feasible or effective.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe2 = response(qna_system_message + \" \" + query2, max_tokens=256).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "GXl09pFfRPBr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Appendicitis symptoms: sudden pain in lower right abdomen, loss of appetite, nausea, vomiting\n",
      "2. Appendicitis diagnosis may include pelvic exam, blood test for white blood cell count elevation, imaging studies like CT scan\n",
      "3. Appendicitis can't be cured with medicine - it requires appendectomy, a surgical procedure to remove the appendix\n",
      "4. Appendix is usually removed through a small incision using laparoscopic surgery or an open procedure through a larger incision depending on severity.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe2 = response(qna_system_message + \" \" + query2, max_tokens=256, temperature=5).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Increasing the temperature parameter has made the model's responses more non-deterministic. This is evident in the following:  \n",
    "\n",
    "- Some points in the response are overly brief, sometimes consisting of just a single word. Sometimes not answering the question at all \n",
    "\n",
    "This behavior highlights the trade-off between creativity and consistency when adjusting the temperature setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRp92JQZOeff"
   },
   "source": [
    "### Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Minoxidil: Over-the-counter topical treatment that stimulates hair growth and slows down hair loss in affected areas.\n",
      "2. Finasteride: Prescription medication that blocks DHT production, reducing hair loss and promoting regrowth.\n",
      "3. Corticosteroids: Topical or oral treatments to reduce inflammation and suppress the immune system, helping with alopecia areata.\n",
      "4. Hair transplant: Surgical procedure where healthy hair follicles are moved from one area of the scalp to another to replace lost hair. Possible causes: stress, autoimmune disorders, hormonal imbalances, nutritional deficiencies.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe3 = response(qna_system_message + \" \" + query3, max_tokens=256).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JOgATEpMRPve"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Minoxidil: A topical medication applied directly to the affected area, promoting hair growth and reducing hair loss.\n",
      "2. Finasteride: An oral medication that inhibits the conversion of testosterone to dihydrotestosterone, which can cause hair loss.\n",
      "3. Corticosteroids: Topical or injected steroids can help reduce inflammation and suppress the immune system response, which may be contributing to the hair loss.\n",
      "4. Identify causes: Possible causes include alopecia areata, nutritional deficiencies, stress, or autoimmune disorders; proper diagnosis is essential for effective treatment.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe3 = response(qna_system_message + \" \" + query3, max_tokens=256, temperature= 5, top_p=0.4).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- When `top_p` is set low (e.g., 0.4), the model only considers the most probable tokens, often resulting in shorter, less detailed, or less varied answers. This can cause the model to omit less common details—like \"possible causes\"—and focus only on the most typical treatments or facts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA45zwyUOefg"
   },
   "source": [
    "### Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Rehabilitation therapy: Occupational, speech, and physical therapy help improve function and compensate for deficits.\n",
      "2. Medications: Anti-seizure drugs, pain relievers, and psychotropic medications may be prescribed to manage symptoms.\n",
      "3. Surgery: Depending on the injury's location and severity, surgery might be necessary to remove hematomas or repair damaged tissue.\n",
      "4. Assistive devices: Devices like wheelchairs, walkers, or communication aids can help individuals with mobility or communication impairments.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe4 = response(qna_system_message + \" \" + query4, max_tokens=256 ).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "VA7G8FOnRQZY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Rehabilitation therapy: Occupational, speech, and physical therapy help improve function and compensate for deficits.\n",
      "2. Medications: Anti-seizure drugs, pain relievers, and psychotropic medications may be prescribed to manage symptoms.\n",
      "3. Surgery: Depending on the injury's location and severity, surgery might be necessary to remove hematomas or repair damaged tissue.\n",
      "4. Assistive devices: Devices like wheelchairs, walkers, or communication aids can help individuals with mobility or communication impairments.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe4 = response(qna_system_message + \" \" + query4, max_tokens=256, top_k=1).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- In our case the model is generating deterministic responses, changing `top_k` may not have any effect. (mostly because we set temp to zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYXxiSuBOefg"
   },
   "source": [
    "### Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "mE2GMQk8RQ_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with prompt engineering:\n",
      "1. Immediate first aid: Apply a sterile dressing to control bleeding, immobilize the affected leg with a splint or sling.\n",
      "2. Seek medical attention: Transport the person to the nearest hospital for proper diagnosis and treatment of the fracture.\n",
      "3. Pain management: Administer over-the-counter pain relievers or prescribed medication as directed by a healthcare professional.\n",
      "4. Rest, ice, compression, elevation (RICE): Follow this protocol to reduce swelling and promote healing after the fracture has been set and immobilized.\n"
     ]
    }
   ],
   "source": [
    "response_text_pe5 = response(qna_system_message + \" \" + query5, max_tokens=256 ).strip()\n",
    "print(\"Response with prompt engineering:\")\n",
    "print(response_text_pe5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - Second Approach - After prompt Engineering and fine tuning\n",
    "The chosen parameters for a deterministic model configuration are:\n",
    "\n",
    "- **max_tokens=256**: Limits the response length to a maximum of 256 tokens, ensuring concise and controlled outputs.\n",
    "- **temperature=0**: Eliminates randomness, making the models responses deterministic and consistent.\n",
    "- **top_p=0.95**: Enables nucleus sampling, considering only the top 95% of the probability mass for token selection, ensuring relevance while maintaining some flexibility.\n",
    "- **top_k=50**: Restricts token selection to the top 50 most probable tokens, further refining the response quality.\n",
    "\n",
    "This configuration is ideal for generating precise, repeatable, and contextually accurate outputs, especially in critical applications like medical question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_O1PGdNO2M9"
   },
   "source": [
    "## Data Preparation for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTpWESc53dL9"
   },
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ybj2cEnzRSXq"
   },
   "outputs": [],
   "source": [
    "#load the pdf file\n",
    "loader = PyMuPDFLoader(\"medical_diagnosis_manual.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffj0ca3eZT4u"
   },
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9weTDzMxRRS"
   },
   "source": [
    "#### Checking the first 2 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "MSEiL--bRTZT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Content:\n",
      "hovels_bounces.0v@icloud.com\n",
      "Y74OHC3EQZ\n",
      "for personal use by hovels_bounces.0v@\n",
      "shing the contents in part or full is liable \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Page 2 Content:\n",
      "hovels_bounces.0v@icloud.com\n",
      "Y74OHC3EQZ\n",
      "This file is meant for personal use by hovels_bounces.0v@icloud.com only.\n",
      "Sharing or publishing the contents in part or full is liable for legal action.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display the content of the first 2 pages\n",
    "for i, doc in enumerate(documents[:2]):\n",
    "    print(f\"Page {i + 1} Content:\\n{doc.page_content}\\n{'-' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-wNNalNxPKT"
   },
   "source": [
    "#### Checking the number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-NuC-6SNRT7K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in the PDF: 4114\n"
     ]
    }
   ],
   "source": [
    "# Check the number of pages in the loaded PDF document\n",
    "num_pages = len(documents)\n",
    "print(f\"Number of pages in the PDF: {num_pages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LECMxTH-zB-R"
   },
   "source": [
    "### Data Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ir9Zi8rKRUmG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks created: 18088\n",
      "Sample chunk:\n",
      " hovels_bounces.0v@icloud.com\n",
      "Y74OHC3EQZ\n",
      "for personal use by hovels_bounces.0v@\n",
      "shing the contents in part or full is liable\n"
     ]
    }
   ],
   "source": [
    "# Chunk the documents for embedding using RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # Number of characters per chunk\n",
    "    chunk_overlap=200     # Overlap between chunks to preserve context\n",
    ")\n",
    "\n",
    "# Split all documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks created: {len(chunks)}\")\n",
    "print(\"Sample chunk:\\n\", chunks[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunk:\n",
      " 201, 910\n",
      "mastocytosis vs 1125\n",
      "Menetrier's disease vs 132\n",
      "peptic ulcer disease vs 134\n",
      "Zolmitriptan 1721\n",
      "Zolpidem 1709, 3103\n",
      "Zonisamide 1701\n",
      "Zoonotic diseases, cutaneous 718\n",
      "Zoophobia 1498\n",
      "Zoster (see Herpes zoster virus infection)\n",
      "Zygomycosis 1332\n",
      "The Merck Manual of Diagnosis & Therapy, 19th Edition\n",
      "Z\n",
      "4104\n",
      "hovels_bounces.0v@icloud.com\n",
      "Y74OHC3EQZ\n",
      "This file is meant for personal use by hovels_bounces.0v@icloud.com only.\n",
      "Sharing or publishing the contents in part or full is liable for legal action.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample chunk:\\n\", chunks[-1].page_content[-500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvHVejcWz0Bl"
   },
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "R3CAgoUeRVLa"
   },
   "outputs": [],
   "source": [
    "def embed_and_create_chroma(chunks, persist_directory=\"chroma_db\", embedding_model_name=\"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Embeds the provided chunks and creates a persistent Chroma vector store.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of langchain Document objects to embed.\n",
    "        persist_directory (str): Directory to persist the Chroma database.\n",
    "        embedding_model_name (str): Name of the sentence-transformer model to use for embeddings.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: A persistent Chroma vector store instance.\n",
    "    \"\"\"\n",
    "    # Create embedding function\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "    # Create Chroma vector store and persist it\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding_function,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiKCOv4X0d7B"
   },
   "source": [
    "### Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jithinravi/work/python/ut_ai_ml/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "# Create and persist the Chroma vector store\n",
    "persist_directory = \"chroma_db\"  # Directory to store the Chroma database\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"  # Name of the embedding model\n",
    "\n",
    "vectordb = embed_and_create_chroma(chunks, persist_directory=persist_directory, embedding_model_name=embedding_model_name)\n",
    "\n",
    "print(\"Chroma vector store created and persisted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks created: 18088\n"
     ]
    }
   ],
   "source": [
    "# Check the number of documents (chunks) stored in the Chroma vector database  vs Original chunks\n",
    "print(f\"Total number of chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks in the vector database: 18088\n"
     ]
    }
   ],
   "source": [
    "# Check the number of documents (chunks) stored in the Chroma vector database\n",
    "print(\"Number of chunks in the vector database:\", vectordb._collection.count())\n",
    "\n",
    "# Optionally, check a few sample chunks stored in the vector database\n",
    "sample_docs = vectordb.get(include=['documents'], ids=[str(i) for i in range(5)])\n",
    "for idx, doc in enumerate(sample_docs['documents']):\n",
    "    print(f\"Chunk {idx}:\\n{doc[:500]}\\n{'-'*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEa5sKc41T1z"
   },
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBlQUGx3RWUD"
   },
   "outputs": [],
   "source": [
    "# Create a retriever from the Chroma vector database\n",
    "retriever = vectordb.as_retriever(  search_type='similarity',search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='16 - Critical Care Medicine\\nChapter 222. Approach to the Critically Ill Patient\\nIntroduction\\nCritical care medicine specializes in caring for the most seriously ill patients. These patients are best\\ntreated in an ICU staffed by experienced personnel. Some hospitals maintain separate units for special\\npopulations (eg, cardiac, surgical, neurologic, pediatric, or neonatal patients). ICUs have a high\\nnurse:patient ratio to provide the necessary high intensity of service, including treatment and monitoring\\nof physiologic parameters.\\nSupportive care for the ICU patient includes provision of adequate nutrition (see p. 21) and prevention of\\ninfection, stress ulcers and gastritis (see p. 131), and pulmonary embolism (see p. 1920). Because 15 to\\n25% of patients admitted to ICUs die there, physicians should know how to minimize suffering and help\\ndying patients maintain dignity (see p. 3480).\\nPatient Monitoring and Testing', metadata={'author': '', 'creationDate': 'D:20120615054440Z', 'creator': 'Atop CHM to PDF Converter', 'file_path': 'medical_diagnosis_manual.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': 'D:20250628125758Z', 'page': 2400, 'producer': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'source': 'medical_diagnosis_manual.pdf', 'subject': '', 'title': 'The Merck Manual of Diagnosis & Therapy, 19th Edition', 'total_pages': 4114, 'trapped': ''}),\n",
       " Document(page_content=\"septic shock is high (60 to 65%). Prognosis depends on the cause, preexisting or complicating illness,\\ntime between onset and diagnosis, and promptness and adequacy of therapy.\\nGeneral management: First aid involves keeping the patient warm. Hemorrhage is controlled, airway\\nand ventilation are checked, and respiratory assistance is given if necessary. Nothing is given by mouth,\\nand the patient's head is turned to one side to avoid aspiration if emesis occurs.\\nTreatment begins simultaneously with evaluation. Supplemental O2 by face mask is provided. If shock is\\nsevere or if ventilation is inadequate, airway intubation with mechanical ventilation is necessary. Two\\nlarge (16- to 18-gauge) IV catheters are inserted into separate peripheral veins. A central venous line or\\nan intraosseous needle, especially in children, provides an alternative when peripheral veins cannot\\npromptly be accessed (see also p. 2250).\", metadata={'author': '', 'creationDate': 'D:20120615054440Z', 'creator': 'Atop CHM to PDF Converter', 'file_path': 'medical_diagnosis_manual.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': 'D:20250628125758Z', 'page': 2448, 'producer': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'source': 'medical_diagnosis_manual.pdf', 'subject': '', 'title': 'The Merck Manual of Diagnosis & Therapy, 19th Edition', 'total_pages': 4114, 'trapped': ''}),\n",
       " Document(page_content=\"may be used. Monotherapy with maximal therapeutic doses of ceftazidime (2 g IV q 8 h) or imipenem (1 g\\nIV q 6 h) may be effective but is not recommended.\\nVancomycin must be added if resistant staphylococci or enterococci are suspected. If there is an\\nabdominal source, a drug effective against anaerobes (eg, metronidazole) should be included. When\\nculture and sensitivity results are available, the antibiotic regimen is changed accordingly. Antibiotics are\\ncontinued for at least 5 days after shock resolves and evidence of infection subsides.\\nAbscesses must be drained, and necrotic tissues (eg, infarcted bowel, gangrenous gall-bladder,\\nabscessed uterus) must be surgically excised. The patient's condition will continue to deteriorate despite\\nantibiotic therapy unless septic foci are eliminated.\\nNormalization of blood glucose improves outcome in critically ill patients, even those not known to be\", metadata={'author': '', 'creationDate': 'D:20120615054440Z', 'creator': 'Atop CHM to PDF Converter', 'file_path': 'medical_diagnosis_manual.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': 'D:20250628125758Z', 'page': 2456, 'producer': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'source': 'medical_diagnosis_manual.pdf', 'subject': '', 'title': 'The Merck Manual of Diagnosis & Therapy, 19th Edition', 'total_pages': 4114, 'trapped': ''})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"How to treat a patient with sepsis in a critical care unit?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vw8qcwq66B0C",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### System and User Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GF_4399TRW5D"
   },
   "outputs": [],
   "source": [
    "# System and user prompt templates for RAG-based medical assistant\n",
    "\n",
    "qna_system_message = (\n",
    "    \"You are a highly knowledgeable medical assistant. \"\n",
    "    \"Use only the provided context from the Merck Manual to answer the user's question. \"\n",
    "    \"Be concise, accurate, and cite relevant sections if possible. \"\n",
    "    \"If the answer is not present in the context, say 'I don't know'. \"\n",
    "    \"Limit your response to a maximum of 4 bullet points, each not exceeding 30 words.\"\n",
    ")\n",
    "\n",
    "qna_user_message_template = (\n",
    "    \"Context:\\n{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    "    \"Answer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkIteX4m6mny"
   },
   "source": [
    "### Response Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jFvGnOJRXZx"
   },
   "outputs": [],
   "source": [
    "def generate_rag_response(user_input,k=3,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
    "    global qna_system_message,qna_user_message_template\n",
    "    # Retrieve relevant document chunks\n",
    "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=k)\n",
    "    context_list = [d.page_content for d in relevant_document_chunks]\n",
    "\n",
    "    # Combine document chunks into a single context\n",
    "    context_for_query = \". \".join(context_list)\n",
    "\n",
    "    user_message = qna_user_message_template.replace('{context}', context_for_query)\n",
    "    user_message = user_message.replace('{question}', user_input)\n",
    "\n",
    "    prompt = qna_system_message + '\\n' + user_message\n",
    "\n",
    "    #print(\"Prompt for LLM:\\n\", prompt)\n",
    "\n",
    "    # Generate the response\n",
    "    try:\n",
    "        response = lcpp_llm(\n",
    "                  prompt=prompt,\n",
    "                  max_tokens=max_tokens,\n",
    "                  temperature=temperature,\n",
    "                  top_p=top_p,\n",
    "                  top_k=top_k\n",
    "                  )\n",
    "\n",
    "        # Extract and print the model's response\n",
    "        response = response['choices'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        response = f'Sorry, I encountered the following error: \\n {e}'\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffP1SRYbPQHN"
   },
   "source": [
    "## Question Answering using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjajBEj06B0E"
   },
   "source": [
    "### Query 1: What is the protocol for managing sepsis in a critical care unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nlo9sMpPRbTP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG:\n",
      "1. Provide supplemental oxygen and secure airway if necessary through intubation and mechanical ventilation.\n",
      "2. Insert two large IV catheters into separate peripheral veins or use a central venous line or intraosseous needle for access.\n",
      "3. Initiate aggressive fluid resuscitation.\n",
      "4. Administer antibiotics promptly based on culture results and suspected organism, and consider surgical intervention for infected or necrotic tissues.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    98.57 ms /   102 runs   (    0.97 ms per token,  1034.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8641.84 ms /   828 tokens (   10.44 ms per token,    95.81 tokens per second)\n",
      "llama_print_timings:        eval time =  6523.88 ms /   101 runs   (   64.59 ms per token,    15.48 tokens per second)\n",
      "llama_print_timings:       total time = 15497.53 ms\n"
     ]
    }
   ],
   "source": [
    "response_text_rag1 = generate_rag_response(query1,k=3,max_tokens=256,temperature=0,top_p=0.95,top_k=50)\n",
    "print(\"Response with RAG:\")\n",
    "print(response_text_rag1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDw8zXuq6B0F"
   },
   "source": [
    "### Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVReF4G8RbzR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG:\n",
      "1. Common symptoms include epigastric or periumbilical pain followed by brief nausea, vomiting, anorexia, and a shift in pain to the right lower quadrant after a few hours. Pain increases with cough and motion. Direct and rebound tenderness are located at McBurney's point.\n",
      "2. Appendicitis cannot be cured via medicine; it requires surgical removal.\n",
      "3. The standard surgical procedure for appendicitis is an appendectomy, which involves removing the inflamed appendix.\n",
      "4. Before the operation, an NGT is inserted, and patients with signs of volume depletion should have urine output monitored with a catheter. IV antibiotics effective against intestinal flora are given to maintain fluid status and prevent infection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =   110.27 ms /   174 runs   (    0.63 ms per token,  1577.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5597.57 ms /   796 tokens (    7.03 ms per token,   142.20 tokens per second)\n",
      "llama_print_timings:        eval time = 10997.95 ms /   173 runs   (   63.57 ms per token,    15.73 tokens per second)\n",
      "llama_print_timings:       total time = 16883.49 ms\n"
     ]
    }
   ],
   "source": [
    "response_text_rag2 = generate_rag_response(query2,k=3,max_tokens=256,temperature=0,top_p=0.95,top_k=50)\n",
    "print(\"Response with RAG:\")\n",
    "print(response_text_rag2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TggYyQPL6B0G"
   },
   "source": [
    "### Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aRbadGtRcX0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG:\n",
      "1. Sudden patchy hair loss can be caused by various conditions such as fungal infections (tinea capitis), bacterial infections, or autoimmune disorders like alopecia areata.\n",
      "2. For fungal infections, topical or oral antifungals are effective treatments.\n",
      "3. For alopecia areata, treatment options include topical corticosteroids, minoxidil, anthralin, immunotherapy (diphencyprone or squaric acid dibutylester), or psoralen plus ultraviolet A (PUVA).\n",
      "4. In cases of trichotillomania, behavior modification, clomipramine, or selective serotonin reuptake inhibitors (SSRIs) may be beneficial.\n",
      "\n",
      "Note: The provided context does not mention any surgical options for treating sudden patchy hair loss specifically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =   124.86 ms /   197 runs   (    0.63 ms per token,  1577.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6098.52 ms /   855 tokens (    7.13 ms per token,   140.20 tokens per second)\n",
      "llama_print_timings:        eval time = 12476.78 ms /   196 runs   (   63.66 ms per token,    15.71 tokens per second)\n",
      "llama_print_timings:       total time = 18892.20 ms\n"
     ]
    }
   ],
   "source": [
    "response_text_rag3 = generate_rag_response(query3,k=3,max_tokens=256,temperature=0,top_p=0.95,top_k=50)\n",
    "print(\"Response with RAG:\")\n",
    "print(response_text_rag3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TgxdI-_6B0G"
   },
   "source": [
    "### Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vzRX1TcRc29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG:\n",
      "1. Ensure reliable airway and maintain adequate ventilation, oxygenation, and blood pressure.\n",
      "2. Surgery may be needed for monitoring intracranial pressure, decompression, or hematoma removal.\n",
      "3. In the first few days, maintain adequate brain perfusion and oxygenation to prevent complications.\n",
      "4. Subsequently, rehabilitation is often required to improve functional recovery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    53.43 ms /    85 runs   (    0.63 ms per token,  1590.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4880.26 ms /   689 tokens (    7.08 ms per token,   141.18 tokens per second)\n",
      "llama_print_timings:        eval time =  5327.22 ms /    84 runs   (   63.42 ms per token,    15.77 tokens per second)\n",
      "llama_print_timings:       total time = 10340.30 ms\n"
     ]
    }
   ],
   "source": [
    "response_text_rag4 = generate_rag_response(query4,k=3,max_tokens=256,temperature=0,top_p=0.95,top_k=50)\n",
    "print(\"Response with RAG:\")\n",
    "print(response_text_rag4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlHXYCkm6B0H"
   },
   "source": [
    "### Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sarpUibcRdhq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG:\n",
      "1. Treat life-threatening injuries first, such as excessive bleeding or signs of shock.\n",
      "2. Immobilize the injured leg using a splint to prevent further damage.\n",
      "3. For lower-limb prosthesis users, maintain body alignment and adjust fit as needed to prevent skin breakdown.\n",
      "4. Seek medical attention for hip fracture surgery and rehabilitation, including walking aids or orthotics for recovery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    58.78 ms /    93 runs   (    0.63 ms per token,  1582.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5358.41 ms /   760 tokens (    7.05 ms per token,   141.83 tokens per second)\n",
      "llama_print_timings:        eval time =  5871.22 ms /    92 runs   (   63.82 ms per token,    15.67 tokens per second)\n",
      "llama_print_timings:       total time = 11376.05 ms\n"
     ]
    }
   ],
   "source": [
    "response_text_rag5 = generate_rag_response(query5,k=3,max_tokens=256,temperature=0,top_p=0.95,top_k=50)\n",
    "print(\"Response with RAG:\")\n",
    "print(response_text_rag5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7TYrqycEITB"
   },
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UYBR-hcReSo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG FT:\n",
      "1. Provide supplemental oxygen and secure airway if necessary through intubation and mechanical ventilation.\n",
      "2. Insert two large IV catheters into separate peripheral veins or use a central venous line or intraosseous needle for access.\n",
      "3. Initiate aggressive fluid resuscitation.\n",
      "4. Administer antibiotics promptly based on culture results and suspected organism, and consider surgical intervention for infected or necrotic tissues.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    68.85 ms /   102 runs   (    0.68 ms per token,  1481.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5382.90 ms /   751 tokens (    7.17 ms per token,   139.52 tokens per second)\n",
      "llama_print_timings:        eval time =  6636.87 ms /   101 runs   (   65.71 ms per token,    15.22 tokens per second)\n",
      "llama_print_timings:       total time = 12199.76 ms\n"
     ]
    }
   ],
   "source": [
    "response_text_rag1_ft = generate_rag_response(query1,max_tokens=256, top_k=20) ## moving top_k to 20 to make it deterministic\n",
    "print(\"Response with RAG FT:\")\n",
    "print(response_text_rag1_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG FT:\n",
      "1. Common symptoms include epigastric or periumbilical pain followed by brief nausea, vomiting, anorexia, and a shift in pain to the right lower quadrant after a few hours. Pain increases with cough and motion. Direct and rebound tenderness are located at McBurney's point.\n",
      "2. Appendicitis cannot be cured via medicine; it requires surgical removal.\n",
      "3. The standard surgical procedure for appendicitis is an appendectomy, which involves removing the inflamed appendix.\n",
      "4. Before the operation, an NGT is inserted, and patients with signs of volume depletion should have urine output monitored with a catheter. IV antibiotics effective against intestinal flora are given to maintain fluid status and prevent infection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =   109.58 ms /   174 runs   (    0.63 ms per token,  1587.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5678.07 ms /   796 tokens (    7.13 ms per token,   140.19 tokens per second)\n",
      "llama_print_timings:        eval time = 11093.37 ms /   173 runs   (   64.12 ms per token,    15.59 tokens per second)\n",
      "llama_print_timings:       total time = 17052.90 ms\n"
     ]
    }
   ],
   "source": [
    "response_text_rag2_ft = generate_rag_response(query2,max_tokens= 256, top_k=20) ## moving top_k to 20 to make it deterministic\n",
    "print(\"Response with RAG FT:\")\n",
    "print(response_text_rag2_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG FT:\n",
      "1. Sudden patchy hair loss can be caused by various conditions such as fungal infections (tinea capitis), bacterial infections, or autoimmune disorders like alopecia areata.\n",
      "2. For fungal infections, topical or oral antifungals are effective treatments.\n",
      "3. For alopecia areata, treatment options include topical corticosteroids, minoxidil, anthralin, immunotherapy (diphencyprone or squaric acid dibutylester), or psoralen plus ultraviolet A (PUVA).\n",
      "4. In cases of trichotillomania, behavior modification, clomipramine, or selective serotonin reuptake inhibitors (SSRIs) may be beneficial.\n",
      "\n",
      "Note: The provided context does not mention any surgical options for treating sudden patchy hair loss specifically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =   130.15 ms /   197 runs   (    0.66 ms per token,  1513.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6120.32 ms /   855 tokens (    7.16 ms per token,   139.70 tokens per second)\n",
      "llama_print_timings:        eval time = 12597.85 ms /   196 runs   (   64.27 ms per token,    15.56 tokens per second)\n",
      "llama_print_timings:       total time = 19056.01 ms\n"
     ]
    }
   ],
   "source": [
    "response_text_rag3_ft = generate_rag_response(query3,max_tokens=256,top_k=20) ## moving top_k to 20 to make it deterministic\n",
    "print(\"Response with RAG FT:\")\n",
    "print(response_text_rag3_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG FT:\n",
      "1. Ensure reliable airway and maintain adequate ventilation, oxygenation, and blood pressure.\n",
      "2. Surgery may be needed for monitoring intracranial pressure, decompression, or hematoma removal.\n",
      "3. In the first few days, maintain adequate brain perfusion and oxygenation to prevent complications.\n",
      "4. Subsequently, rehabilitation is often required to improve functional recovery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    53.62 ms /    85 runs   (    0.63 ms per token,  1585.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4878.83 ms /   689 tokens (    7.08 ms per token,   141.22 tokens per second)\n",
      "llama_print_timings:        eval time =  5317.33 ms /    84 runs   (   63.30 ms per token,    15.80 tokens per second)\n",
      "llama_print_timings:       total time = 10331.82 ms\n"
     ]
    }
   ],
   "source": [
    "response_text_rag4_ft = generate_rag_response(query4,max_tokens=256,top_k=20) ## moving top_k to 20 to make it deterministic\n",
    "print(\"Response with RAG FT:\")\n",
    "print(response_text_rag4_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with RAG FT:\n",
      "1. Treat life-threatening injuries first, such as excessive bleeding or signs of shock.\n",
      "2. Immobilize the injured leg using a splint to prevent further damage.\n",
      "3. For lower-limb prosthesis users, maintain body alignment and adjust fit as needed to prevent skin breakdown.\n",
      "4. Seek medical attention for hip fracture surgery and rehabilitation, including walking aids or orthotics for recovery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    62.26 ms /    93 runs   (    0.67 ms per token,  1493.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5345.83 ms /   760 tokens (    7.03 ms per token,   142.17 tokens per second)\n",
      "llama_print_timings:        eval time =  6014.52 ms /    92 runs   (   65.38 ms per token,    15.30 tokens per second)\n",
      "llama_print_timings:       total time = 11537.44 ms\n"
     ]
    }
   ],
   "source": [
    "response_text_rag5_ft = generate_rag_response(query5,max_tokens=256,top_k=20) ## moving top_k to 20 to make it deterministic\n",
    "print(\"Response with RAG FT:\")\n",
    "print(response_text_rag5_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyQrTipNfuBN"
   },
   "source": [
    "## Output Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHbfLAxAGdhW"
   },
   "outputs": [],
   "source": [
    "groundedness_rater_system_message = (\n",
    "    \"You are a medical expert evaluating the groundedness of an AI-generated answer. \"\n",
    "    \"Given the context from the Merck Manual and the answer, rate how well the answer is supported by the provided context. \"\n",
    "    \"Respond with one of: 'Fully Grounded', 'Partially Grounded', or 'Not Grounded'. \"\n",
    "    \"And in the next line, Briefly justify your rating in 1-2 sentences.\"\n",
    "    \"Sample answer: Rating: Fully Grounded \\\n",
    "     Justification: The answer is fully supported by the context provided, citing specific sections of the Merck Manual.\"\n",
    ")\n",
    "\n",
    "relevance_rater_system_message = (\n",
    "    \"You are a medical expert evaluating the relevance of an AI-generated answer. \"\n",
    "    \"Given the user's question and the answer, rate how relevant the answer is to the question. \"\n",
    "    \"Respond with one of: 'Highly Relevant', 'Somewhat Relevant', or 'Not Relevant'. \"\n",
    "    \"And in the next line, briefly justify your rating in 1-2 sentences.\"\n",
    "    \"Sample answer: Rating: Higly Relevant \\\n",
    "    Justification:  The answer directly addresses the user's question, providing specific and actionable information.\"\n",
    ")\n",
    "\n",
    "user_message_template = (\n",
    "    \"Context:\\n{context}\\n\\n\"\n",
    "    \"Answer:\\n{answer}\\n\\n\"\n",
    "    \"Question:\\n{question}\\n\\n\"\n",
    "    \"Please provide your rating and justification.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIbZybyuRi2p"
   },
   "outputs": [],
   "source": [
    "def generate_ground_relevance_response(user_input,k=3,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
    "    global qna_system_message,qna_user_message_template\n",
    "    # Retrieve relevant document chunks\n",
    "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=3)\n",
    "    pages = [str(doc.metadata.get('page', 'N/A')) for doc in relevant_document_chunks]\n",
    "    pages_str = \", \".join(pages)\n",
    "\n",
    "    context_list = [d.page_content for d in relevant_document_chunks]\n",
    "    context_for_query = \". \".join(context_list)\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    prompt = f\"\"\"[INST]{qna_system_message}\\n\n",
    "                {'user'}: {qna_user_message_template.format(context=context_for_query, question=user_input)}\n",
    "                [/INST]\"\"\"\n",
    "\n",
    "    response = lcpp_llm(\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            stop=['INST'],\n",
    "            echo=False\n",
    "            )\n",
    "\n",
    "    answer =  response[\"choices\"][0][\"text\"]\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    groundedness_prompt = f\"\"\"[INST]{groundedness_rater_system_message}\\n\n",
    "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n",
    "                [/INST]\"\"\"\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    relevance_prompt = f\"\"\"[INST]{relevance_rater_system_message}\\n\n",
    "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n",
    "                [/INST]\"\"\"\n",
    "\n",
    "    response_1 =  lcpp_llm(\n",
    "            prompt=groundedness_prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            stop=['INST'],\n",
    "            echo=False\n",
    "            )\n",
    "\n",
    "    response_2 = lcpp_llm(\n",
    "            prompt=relevance_prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            stop=['INST'],\n",
    "            echo=False\n",
    "            )\n",
    "\n",
    "    return response_1['choices'][0]['text'],response_2['choices'][0]['text'],pages_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    48.89 ms /    76 runs   (    0.64 ms per token,  1554.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5921.23 ms /   852 tokens (    6.95 ms per token,   143.89 tokens per second)\n",
      "llama_print_timings:        eval time =  4769.98 ms /    75 runs   (   63.60 ms per token,    15.72 tokens per second)\n",
      "llama_print_timings:       total time = 10811.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    75.09 ms /   119 runs   (    0.63 ms per token,  1584.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7040.76 ms /   972 tokens (    7.24 ms per token,   138.05 tokens per second)\n",
      "llama_print_timings:        eval time =  7528.65 ms /   118 runs   (   63.80 ms per token,    15.67 tokens per second)\n",
      "llama_print_timings:       total time = 14760.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    92.06 ms /    80 runs   (    1.15 ms per token,   869.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7856.19 ms /   956 tokens (    8.22 ms per token,   121.69 tokens per second)\n",
      "llama_print_timings:        eval time =  5150.57 ms /    79 runs   (   65.20 ms per token,    15.34 tokens per second)\n",
      "llama_print_timings:       total time = 13311.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =   104.90 ms /   162 runs   (    0.65 ms per token,  1544.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6226.44 ms /   891 tokens (    6.99 ms per token,   143.10 tokens per second)\n",
      "llama_print_timings:        eval time = 10226.56 ms /   161 runs   (   63.52 ms per token,    15.74 tokens per second)\n",
      "llama_print_timings:       total time = 16720.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    46.18 ms /    72 runs   (    0.64 ms per token,  1559.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8182.10 ms /  1103 tokens (    7.42 ms per token,   134.81 tokens per second)\n",
      "llama_print_timings:        eval time =  4562.99 ms /    71 runs   (   64.27 ms per token,    15.56 tokens per second)\n",
      "llama_print_timings:       total time = 12863.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    60.04 ms /    91 runs   (    0.66 ms per token,  1515.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9171.54 ms /  1087 tokens (    8.44 ms per token,   118.52 tokens per second)\n",
      "llama_print_timings:        eval time =  5729.82 ms /    90 runs   (   63.66 ms per token,    15.71 tokens per second)\n",
      "llama_print_timings:       total time = 15051.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =   183.88 ms /   285 runs   (    0.65 ms per token,  1549.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6734.61 ms /   950 tokens (    7.09 ms per token,   141.06 tokens per second)\n",
      "llama_print_timings:        eval time = 18043.69 ms /   284 runs   (   63.53 ms per token,    15.74 tokens per second)\n",
      "llama_print_timings:       total time = 25259.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    74.96 ms /    71 runs   (    1.06 ms per token,   947.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10325.63 ms /  1285 tokens (    8.04 ms per token,   124.45 tokens per second)\n",
      "llama_print_timings:        eval time =  5005.91 ms /    70 runs   (   71.51 ms per token,    13.98 tokens per second)\n",
      "llama_print_timings:       total time = 15603.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    38.05 ms /    59 runs   (    0.64 ms per token,  1550.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10895.50 ms /  1269 tokens (    8.59 ms per token,   116.47 tokens per second)\n",
      "llama_print_timings:        eval time =  3777.10 ms /    58 runs   (   65.12 ms per token,    15.36 tokens per second)\n",
      "llama_print_timings:       total time = 14773.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    95.52 ms /   147 runs   (    0.65 ms per token,  1538.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5429.93 ms /   784 tokens (    6.93 ms per token,   144.38 tokens per second)\n",
      "llama_print_timings:        eval time =  9377.57 ms /   146 runs   (   64.23 ms per token,    15.57 tokens per second)\n",
      "llama_print_timings:       total time = 15083.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    86.32 ms /   136 runs   (    0.63 ms per token,  1575.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7821.84 ms /   981 tokens (    7.97 ms per token,   125.42 tokens per second)\n",
      "llama_print_timings:        eval time =  8532.46 ms /   135 runs   (   63.20 ms per token,    15.82 tokens per second)\n",
      "llama_print_timings:       total time = 16584.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    40.97 ms /    64 runs   (    0.64 ms per token,  1562.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7336.86 ms /   965 tokens (    7.60 ms per token,   131.53 tokens per second)\n",
      "llama_print_timings:        eval time =  4055.05 ms /    63 runs   (   64.37 ms per token,    15.54 tokens per second)\n",
      "llama_print_timings:       total time = 11496.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =   206.88 ms /   223 runs   (    0.93 ms per token,  1077.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5946.21 ms /   855 tokens (    6.95 ms per token,   143.79 tokens per second)\n",
      "llama_print_timings:        eval time = 14485.13 ms /   222 runs   (   65.25 ms per token,    15.33 tokens per second)\n",
      "llama_print_timings:       total time = 21178.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    32.87 ms /    51 runs   (    0.64 ms per token,  1551.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8484.93 ms /  1128 tokens (    7.52 ms per token,   132.94 tokens per second)\n",
      "llama_print_timings:        eval time =  3239.53 ms /    50 runs   (   64.79 ms per token,    15.43 tokens per second)\n",
      "llama_print_timings:       total time = 11819.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   291.60 ms\n",
      "llama_print_timings:      sample time =    49.85 ms /    57 runs   (    0.87 ms per token,  1143.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9432.30 ms /  1112 tokens (    8.48 ms per token,   117.89 tokens per second)\n",
      "llama_print_timings:        eval time =  3645.51 ms /    56 runs   (   65.10 ms per token,    15.36 tokens per second)\n",
      "llama_print_timings:       total time = 13235.21 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the protocol for managing sepsis in a ...</td>\n",
       "      <td>1. Provide supplemental oxygen and secure airw...</td>\n",
       "      <td>Rating: Fully Grounded\\nJustification: The ans...</td>\n",
       "      <td>Rating: Highly Relevant\\nJustification: The an...</td>\n",
       "      <td>2400, 2448, 2453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the common symptoms for appendicitis,...</td>\n",
       "      <td>1. Common symptoms include epigastric or periu...</td>\n",
       "      <td>Rating: Fully Grounded\\nJustification: The ans...</td>\n",
       "      <td>Rating: Highly Relevant\\nJustification: The an...</td>\n",
       "      <td>173, 172, 3567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the effective treatments or solutions...</td>\n",
       "      <td>1. Sudden patchy hair loss can be caused by va...</td>\n",
       "      <td>Rating: Fully Grounded\\nJustification: The ans...</td>\n",
       "      <td>Rating: Highly Relevant\\nJustification: The an...</td>\n",
       "      <td>858, 855, 857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What treatments are recommended for a person w...</td>\n",
       "      <td>1. Ensure reliable airway and maintain adequat...</td>\n",
       "      <td>Rating: Fully Grounded\\nJustification: The ans...</td>\n",
       "      <td>Rating: Highly Relevant\\nJustification: The an...</td>\n",
       "      <td>3647, 3409, 3404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the necessary precautions and treatme...</td>\n",
       "      <td>1. Treat life-threatening injuries first, such...</td>\n",
       "      <td>Rating: Fully Grounded\\nJustification: The ans...</td>\n",
       "      <td>Rating: Highly Relevant\\nJustification: The an...</td>\n",
       "      <td>3390, 3646, 1023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0  What is the protocol for managing sepsis in a ...   \n",
       "1  What are the common symptoms for appendicitis,...   \n",
       "2  What are the effective treatments or solutions...   \n",
       "3  What treatments are recommended for a person w...   \n",
       "4  What are the necessary precautions and treatme...   \n",
       "\n",
       "                                              Answer  \\\n",
       "0  1. Provide supplemental oxygen and secure airw...   \n",
       "1  1. Common symptoms include epigastric or periu...   \n",
       "2  1. Sudden patchy hair loss can be caused by va...   \n",
       "3  1. Ensure reliable airway and maintain adequat...   \n",
       "4  1. Treat life-threatening injuries first, such...   \n",
       "\n",
       "                                        Groundedness  \\\n",
       "0  Rating: Fully Grounded\\nJustification: The ans...   \n",
       "1  Rating: Fully Grounded\\nJustification: The ans...   \n",
       "2  Rating: Fully Grounded\\nJustification: The ans...   \n",
       "3  Rating: Fully Grounded\\nJustification: The ans...   \n",
       "4  Rating: Fully Grounded\\nJustification: The ans...   \n",
       "\n",
       "                                           Relevance             Pages  \n",
       "0  Rating: Highly Relevant\\nJustification: The an...  2400, 2448, 2453  \n",
       "1  Rating: Highly Relevant\\nJustification: The an...    173, 172, 3567  \n",
       "2  Rating: Highly Relevant\\nJustification: The an...     858, 855, 857  \n",
       "3  Rating: Highly Relevant\\nJustification: The an...  3647, 3409, 3404  \n",
       "4  Rating: Highly Relevant\\nJustification: The an...  3390, 3646, 1023  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for q, ans in [\n",
    "    (query1, response_text_rag1_ft),\n",
    "    (query2, response_text_rag2_ft),\n",
    "    (query3, response_text_rag3_ft),\n",
    "    (query4, response_text_rag4_ft),\n",
    "    (query5, response_text_rag5_ft),\n",
    "]:\n",
    "    ground, rel, page_str = generate_ground_relevance_response(user_input=q, max_tokens=370, top_k=20)\n",
    "    results.append({\n",
    "        \"Query\": q,\n",
    "        \"Answer\": ans,\n",
    "        \"Groundedness\": ground.strip(),\n",
    "        \"Relevance\": rel.strip(),\n",
    "        \"Pages\": page_str if \"Not Grounded\" not in ground else None\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(results)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query 1:** What is the protocol for managing sepsis in a critical care unit?\n",
       "\n",
       "**Answer:**\n",
       "1. Provide supplemental oxygen and secure airway if necessary through intubation and mechanical ventilation.\n",
       "2. Insert two large IV catheters into separate peripheral veins or use a central venous line or intraosseous needle for access.\n",
       "3. Initiate aggressive fluid resuscitation.\n",
       "4. Administer antibiotics promptly based on culture results and suspected organism, and consider surgical intervention for infected or necrotic tissues.\n",
       "\n",
       "**Groundedness:** Rating: Fully Grounded\n",
       "Justification: The answer directly aligns with the information provided in the context from the Merck Manual, specifically Chapter 227 on Sepsis and Septic Shock. The manual suggests providing supplemental oxygen, checking airway and ventilation, inserting large IV catheters for fluid resuscitation, initiating aggressive fluid resuscitation, antibiotics, and considering surgical intervention if necessary. Additionally, the answer mentions monitoring for organ failure and providing supportive care as needed, which is also mentioned in the context.\n",
       "\n",
       "**Pages:** 2400, 2448, 2453\n",
       "\n",
       "**Relevance:** Rating: Highly Relevant\n",
       "Justification: The answer directly addresses the user's question by providing specific steps for managing sepsis in a critical care unit, including supplemental oxygen, checking airway and ventilation, inserting large IV catheters for fluid resuscitation, aggressive fluid resuscitation, antibiotics, surgical intervention if necessary, and supportive care.\n",
       "\n",
       "---\n",
       "**Query 2:** What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n",
       "\n",
       "**Answer:**\n",
       "1. Common symptoms include epigastric or periumbilical pain followed by brief nausea, vomiting, anorexia, and a shift in pain to the right lower quadrant after a few hours. Pain increases with cough and motion. Direct and rebound tenderness are located at McBurney's point.\n",
       "2. Appendicitis cannot be cured via medicine; it requires surgical removal.\n",
       "3. The standard surgical procedure for appendicitis is an appendectomy, which involves removing the inflamed appendix.\n",
       "4. Before the operation, an NGT is inserted, and patients with signs of volume depletion should have urine output monitored with a catheter. IV antibiotics effective against intestinal flora are given to maintain fluid status and prevent infection.\n",
       "\n",
       "**Groundedness:** Rating: Fully Grounded\n",
       "Justification: The answer directly addresses the question by accurately quoting the common symptoms of appendicitis from the context provided in the Merck Manual, and also clarifies that appendicitis cannot be cured via medicine and requires surgical removal (appendectomy). The answer is fully supported by the context.\n",
       "\n",
       "**Pages:** 173, 172, 3567\n",
       "\n",
       "**Relevance:** Rating: Highly Relevant\n",
       "Justification: The answer directly addresses the user's question by providing a clear list of common symptoms for appendicitis and stating that it cannot be cured via medicine, but rather requires surgical removal through an appendectomy. The answer also mentions the pre-surgery preparations such as NGT, urinary catheter, IV fluids and electrolytes, and antibiotics.\n",
       "\n",
       "---\n",
       "**Query 3:** What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n",
       "\n",
       "**Answer:**\n",
       "1. Sudden patchy hair loss can be caused by various conditions such as fungal infections (tinea capitis), bacterial infections, or autoimmune disorders like alopecia areata.\n",
       "2. For fungal infections, topical or oral antifungals are effective treatments.\n",
       "3. For alopecia areata, treatment options include topical corticosteroids, minoxidil, anthralin, immunotherapy (diphencyprone or squaric acid dibutylester), or psoralen plus ultraviolet A (PUVA).\n",
       "4. In cases of trichotillomania, behavior modification, clomipramine, or selective serotonin reuptake inhibitors (SSRIs) may be beneficial.\n",
       "\n",
       "Note: The provided context does not mention any surgical options for treating sudden patchy hair loss specifically.\n",
       "\n",
       "**Groundedness:** Rating: Fully Grounded\n",
       "Justification: The answer accurately identifies the causes of sudden patchy hair loss (alopecia areata, tinea capitis, trichotillomania) based on the provided context from the Merck Manual. Additionally, it lists the appropriate treatments for each condition as mentioned in the manual.\n",
       "\n",
       "**Pages:** 858, 855, 857\n",
       "\n",
       "**Relevance:** Rating: Highly Relevant\n",
       "Justification: The answer directly addresses the user's question by providing information about the potential causes of sudden patchy hair loss (alopecia areata, tinea capitis, trichotillomania) and effective treatments for each condition.\n",
       "\n",
       "---\n",
       "**Query 4:** What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n",
       "\n",
       "**Answer:**\n",
       "1. Ensure reliable airway and maintain adequate ventilation, oxygenation, and blood pressure.\n",
       "2. Surgery may be needed for monitoring intracranial pressure, decompression, or hematoma removal.\n",
       "3. In the first few days, maintain adequate brain perfusion and oxygenation to prevent complications.\n",
       "4. Subsequently, rehabilitation is often required to improve functional recovery.\n",
       "\n",
       "**Groundedness:** Rating: Fully Grounded\n",
       "Justification: The answer accurately reflects the information provided in the context from the Merck Manual, which includes ensuring a reliable airway and maintaining adequate ventilation, oxygenation, and blood pressure as initial treatments for traumatic brain injury (p. 3400). Additionally, the answer mentions the need for surgery for patients with more severe injuries to place monitors, decompress the brain, or remove intracranial hematomas (p. 3400), and the importance of rehabilitation for functional recovery and preventing secondary disabilities after initial treatment (p. 3231).\n",
       "\n",
       "**Pages:** 3647, 3409, 3404\n",
       "\n",
       "**Relevance:** Rating: Highly Relevant\n",
       "Justification: The answer directly addresses the user's question by providing specific treatments recommended for a person with a traumatic brain injury, including ensuring a reliable airway, maintaining adequate ventilation, oxygenation, and blood pressure, surgery if necessary, and rehabilitation.\n",
       "\n",
       "---\n",
       "**Query 5:** What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n",
       "\n",
       "**Answer:**\n",
       "1. Treat life-threatening injuries first, such as excessive bleeding or signs of shock.\n",
       "2. Immobilize the injured leg using a splint to prevent further damage.\n",
       "3. For lower-limb prosthesis users, maintain body alignment and adjust fit as needed to prevent skin breakdown.\n",
       "4. Seek medical attention for hip fracture surgery and rehabilitation, including walking aids or orthotics for recovery.\n",
       "\n",
       "**Groundedness:** Rating: Fully Grounded\n",
       "Justification: The answer directly addresses the question by summarizing the necessary precautions and treatment steps for a person who has fractured their leg, drawing from specific sections of the Merck Manual context provided.\n",
       "\n",
       "**Pages:** 3390, 3646, 1023\n",
       "\n",
       "**Relevance:** Rating: Highly Relevant\n",
       "Justification: The answer directly addresses the user's question by providing specific and actionable information on the necessary precautions and treatment steps for a person who has fractured their leg, as well as considerations for their care and recovery.\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Create a more readable summary of the evaluation results, including pages\n",
    "\n",
    "def format_eval_content(df):\n",
    "    content = \"\"\n",
    "    for idx, row in df.iterrows():\n",
    "        content += f\"**Query {idx+1}:** {row['Query']}\\n\\n\"\n",
    "        content += f\"**Answer:**\\n{row['Answer']}\\n\\n\"\n",
    "        content += f\"**Groundedness:** {row['Groundedness']}\\n\\n\"\n",
    "        if 'Pages' in row and pd.notnull(row['Pages']):\n",
    "            content += f\"**Pages:** {row['Pages']}\\n\\n\"\n",
    "        content += f\"**Relevance:** {row['Relevance']}\\n\\n\"\n",
    "        content += \"---\\n\"\n",
    "    return content\n",
    "\n",
    "readable_eval_content = format_eval_content(eval_df)\n",
    "display(Markdown(readable_eval_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actionable Insights and Business Recommendations\n",
    "\n",
    "\n",
    "1. **Improved Contextual Responses**:\n",
    "    - RAG-based approach enhances relevance and groundedness using trusted sources like the Merck Manual.\n",
    "    - Fine-tuned parameters (`temperature=0`, `top_p=0.95`, `top_k=20`) ensure consistent outputs.\n",
    "\n",
    "2. **Efficient Data Handling**:\n",
    "    - Chunking with RecursiveCharacterTextSplitter preserves context, creating 18,088 chunks from 4,114 pages.\n",
    "    - Chroma vector database enables fast, domain-specific retrieval.\n",
    "\n",
    "3. **Evaluation Metrics**:\n",
    "    - Groundedness: 100% Fully Grounded.\n",
    "    - Relevance: 100% Highly Relevant.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Business Recommendations:\n",
    "1. **Healthcare Assistant Product**:\n",
    "    - Develop an AI-powered assistant for clinicians and patients.\n",
    "\n",
    "2. **Expand Knowledge Base**:\n",
    "    - Add sources like PubMed to cover broader medical domains.\n",
    "\n",
    "3. **Regulatory Compliance**:\n",
    "    - Ensure HIPAA/GDPR compliance for data security.\n",
    "\n",
    "4. **Specialized Solutions**:\n",
    "    - Tailor systems for specialties like neurology or cardiology.\n",
    "\n",
    "5. **Continuous Updates**:\n",
    "    - Regularly update the vector database with new medical research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybRlzaIhWaM9"
   },
   "source": [
    "<font size=6 color='blue'>Power Ahead</font>\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3CNz35ia6Bz3",
    "CkRbhMJH6Bz3",
    "CARPKFwm6Bz4",
    "by9EvAnkSpZf",
    "lnwETBOE6Bz5",
    "TtZWqj0wFTS1",
    "Uq1lhM4WFTS2",
    "EzzkvIXvFTS4",
    "K8YgK91SFjVY",
    "J6yxICeVFjVc",
    "oflaoOGiFjVd",
    "WUUqY4FbFjVe",
    "5laPFTHrFjVf",
    "g5myZ5dOOefc",
    "9Jg3r_LWOeff",
    "iYpyw4HjOeff",
    "dRp92JQZOeff",
    "AA45zwyUOefg",
    "TYXxiSuBOefg",
    "t_O1PGdNO2M9",
    "uTpWESc53dL9",
    "ffj0ca3eZT4u",
    "f9weTDzMxRRS",
    "7-wNNalNxPKT",
    "LECMxTH-zB-R",
    "BvHVejcWz0Bl",
    "qiKCOv4X0d7B",
    "uEa5sKc41T1z",
    "vw8qcwq66B0C",
    "TkIteX4m6mny",
    "ffP1SRYbPQHN",
    "JjajBEj06B0E",
    "QDw8zXuq6B0F",
    "TggYyQPL6B0G",
    "1TgxdI-_6B0G",
    "FlHXYCkm6B0H",
    "K7TYrqycEITB",
    "Y7QICRU-njdj"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
